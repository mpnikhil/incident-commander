/* eslint-disable */
/** Complete catalog of all available AI models */
export const MODEL_CATALOG = [
  {
    name: 'smart-turn-v2',
    capability: 'audio',
    provider: 'cloudflare',
    providerModel: '@cf/pipecat-ai/smart-turn-v2',
    description: 'An open source, community-driven, native audio turn detection model in 2nd version'
  },
  {
    name: 'gpt-oss-120b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/openai/gpt-oss-120b',
    description: 'OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases – gpt-oss-120b is for production, general purpose, high reasoning use-cases.'
  },
  {
    name: 'qwen-1.5-0.5b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/qwen/qwen1.5-0.5b-chat',
    description: 'Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud.'
  },
  {
    name: 'bge-m3',
    capability: 'embeddings',
    provider: 'cloudflare',
    providerModel: '@cf/baai/bge-m3',
    description: 'Multi-Functionality, Multi-Linguality, and Multi-Granularity embeddings model.'
  },
  {
    name: 'distilbert-sst-2-int8',
    capability: 'text-classification',
    provider: 'cloudflare',
    providerModel: '@cf/huggingface/distilbert-sst-2-int8',
    description: 'Distilled BERT model that was finetuned on SST-2 for sentiment classification'
  },
  {
    name: 'gemma-2b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/google/gemma-2b-it-lora',
    description: 'This is a Gemma-2B base model that Cloudflare dedicates for inference with LoRA adapters. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.'
  },
  {
    name: 'starling-lm-7b-beta',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/nexusflow/starling-lm-7b-beta',
    description: 'We introduce Starling-LM-7B-beta, an open large language model (LLM) trained by Reinforcement Learning from AI Feedback (RLAIF). Starling-LM-7B-beta is trained from Openchat-3.5-0106 with our new reward model Nexusflow/Starling-RM-34B and policy optimization method Fine-Tuning Language Models from Human Preferences (PPO).'
  },
  {
    name: 'llama-3-8b-instruct',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3-8b-instruct',
    description: 'Generation over generation, Meta Llama 3 demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning.'
  },
  {
    name: 'llama-3.2-3b-instruct',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.2-3b-instruct',
    description: 'The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.'
  },
  {
    name: 'llamaguard-7b-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/llamaguard-7b-awq',
    description: 'Llama Guard is a model for classifying the safety of LLM prompts and responses, using a taxonomy of safety risks. '
  },
  {
    name: 'neural-chat-7b-v3-1-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/neural-chat-7b-v3-1-awq',
    description: 'This model is a fine-tuned 7B parameter LLM on the Intel Gaudi 2 processor from the mistralai/Mistral-7B-v0.1 on the open source dataset Open-Orca/SlimOrca.'
  },
  {
    name: 'llama-guard-3-8b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-guard-3-8b',
    description: 'Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.'
  },
  {
    name: 'llama-2-7b-chat-fp16',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-2-7b-chat-fp16',
    description: 'Full precision (fp16) generative text model with 7 billion parameters from Meta'
  },
  {
    name: 'mistral-7b-instruct',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/mistral/mistral-7b-instruct-v0.1',
    description: 'Instruct fine-tuned version of the Mistral-7b generative text model with 7 billion parameters'
  },
  {
    name: 'melotts',
    capability: 'tts',
    provider: 'cloudflare',
    providerModel: '@cf/myshell-ai/melotts',
    description: 'MeloTTS is a high-quality multi-lingual text-to-speech library by MyShell.ai.'
  },
  {
    name: 'mistral-7b-instruct-v0.2-lora',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/mistral/mistral-7b-instruct-v0.2-lora',
    description: 'The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.'
  },
  {
    name: 'whisper',
    capability: 'audio',
    provider: 'cloudflare',
    providerModel: '@cf/openai/whisper',
    description: 'Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.'
  },
  {
    name: 'tinyllama-1.1b-chat-v1.0',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/tinyllama/tinyllama-1.1b-chat-v1.0',
    description: 'The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. This is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T.'
  },
  {
    name: 'mistral-7b-instruct-v0.2',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/mistral/mistral-7b-instruct-v0.2',
    description: 'The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2. Mistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1: 32k context window (vs 8k context in v0.1), rope-theta = 1e6, and no Sliding-Window Attention.'
  },
  {
    name: 'una-cybertron-7b-v2-bf16',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/fblgit/una-cybertron-7b-v2-bf16',
    description: 'Cybertron 7B v2 is a 7B MistralAI based model, best on it\'s series. It was trained with SFT, DPO and UNA (Unified Neural Alignment) on multiple datasets.'
  },
  {
    name: 'llava-1.5-7b',
    capability: 'vision',
    provider: 'cloudflare',
    providerModel: '@cf/llava-hf/llava-1.5-7b-hf',
    description: 'LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture.'
  },
  {
    name: 'deepseek-r1-distill-qwen-32b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/deepseek-ai/deepseek-r1-distill-qwen-32b',
    description: 'DeepSeek-R1-Distill-Qwen-32B is a model distilled from DeepSeek-R1 based on Qwen2.5. It outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.'
  },
  {
    name: 'stable-diffusion-v1-5-inpainting',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/runwayml/stable-diffusion-v1-5-inpainting',
    description: 'Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.'
  },
  {
    name: 'nova-3',
    capability: 'audio',
    provider: 'cloudflare',
    providerModel: '@cf/deepgram/nova-3',
    description: 'Transcribe audio using Deepgram’s speech-to-text model'
  },
  {
    name: 'flux-1-schnell',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/black-forest-labs/flux-1-schnell',
    description: 'FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. '
  },
  {
    name: 'discolm-german-7b-v1-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/thebloke/discolm-german-7b-v1-awq',
    description: 'DiscoLM German 7b is a Mistral-based large language model with a focus on German-language applications. AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization.'
  },
  {
    name: 'llama-2-7b-chat-int8',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-2-7b-chat-int8',
    description: 'Quantized (int8) generative text model with 7 billion parameters from Meta'
  },
  {
    name: 'llama-3.1-8b-instruct-fp8',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.1-8b-instruct-fp8',
    description: 'Llama 3.1 8B quantized to FP8 precision'
  },
  {
    name: 'mistral-7b-instruct-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/mistral-7b-instruct-v0.1-awq',
    description: 'Mistral 7B Instruct v0.1 AWQ is an efficient, accurate and blazing-fast low-bit weight quantized Mistral variant.'
  },
  {
    name: 'qwen-1.5-7b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/qwen/qwen1.5-7b-chat-awq',
    description: 'Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud. AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization.'
  },
  {
    name: 'llama-3.2-1b-instruct',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.2-1b-instruct',
    description: 'The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.'
  },
  {
    name: 'llama-2-13b-chat-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/llama-2-13b-chat-awq',
    description: 'Llama 2 13B Chat AWQ is an efficient, accurate and blazing-fast low-bit weight quantized Llama 2 variant.'
  },
  {
    name: 'resnet-50',
    capability: 'image-classification',
    provider: 'cloudflare',
    providerModel: '@cf/microsoft/resnet-50',
    description: '50 layers deep image classification CNN trained on more than 1M images from ImageNet'
  },
  {
    name: 'stable-diffusion-xl-lightning',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/bytedance/stable-diffusion-xl-lightning',
    description: 'SDXL-Lightning is a lightning-fast text-to-image generation model. It can generate high-quality 1024px images in a few steps.'
  },
  {
    name: 'deepseek-coder-6.7b-base',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/deepseek-coder-6.7b-base-awq',
    description: 'Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.'
  },
  {
    name: 'llama-2-7b-chat-hf-lora',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta-llama/llama-2-7b-chat-hf-lora',
    description: 'This is a Llama2 base model that Cloudflare dedicated for inference with LoRA adapters. Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. '
  },
  {
    name: 'llama-3.3-70b-instruct-fp8',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.3-70b-instruct-fp8-fast',
    description: 'Llama 3.3 70B quantized to fp8 precision, optimized to be faster.'
  },
  {
    name: 'dreamshaper-8-lcm',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/lykon/dreamshaper-8-lcm',
    description: 'Stable Diffusion model that has been fine-tuned to be better at photorealism without sacrificing range.'
  },
  {
    name: 'phoenix-1.0',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/leonardo/phoenix-1.0',
    description: 'Phoenix 1.0 is a model by Leonardo.Ai that generates images with exceptional prompt adherence and coherent text.'
  },
  {
    name: 'stable-diffusion-xl-base-1.0',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/stabilityai/stable-diffusion-xl-base-1.0',
    description: 'Diffusion-based text-to-image generative model by Stability AI. Generates and modify images based on text prompts.'
  },
  {
    name: 'openhermes-2.5-mistral-7b-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/openhermes-2.5-mistral-7b-awq',
    description: 'OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.'
  },
  {
    name: 'm2m100-1.2b',
    capability: 'translation',
    provider: 'cloudflare',
    providerModel: '@cf/meta/m2m100-1.2b',
    description: 'Multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation'
  },
  {
    name: 'deepseek-coder-6.7b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/deepseek-coder-6.7b-instruct-awq',
    description: 'Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.'
  },
  {
    name: 'bge-small-en',
    capability: 'embeddings',
    provider: 'cloudflare',
    providerModel: '@cf/baai/bge-small-en-v1.5',
    description: 'BAAI general embedding (Small) model that transforms any given text into a 384-dimensional vector'
  },
  {
    name: 'qwen-coder-32b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/qwen/qwen2.5-coder-32b-instruct',
    description: 'Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:'
  },
  {
    name: 'deepseek-math-7b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/deepseek-ai/deepseek-math-7b-instruct',
    description: 'DeepSeekMath-Instruct 7B is a mathematically instructed tuning model derived from DeepSeekMath-Base 7B. DeepSeekMath is initialized with DeepSeek-Coder-v1.5 7B and continues pre-training on math-related tokens sourced from Common Crawl, together with natural language and code data for 500B tokens.'
  },
  {
    name: 'falcon-7b-instruct',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/tiiuae/falcon-7b-instruct',
    description: 'Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets.'
  },
  {
    name: 'hermes-2-pro-mistral-7b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/nousresearch/hermes-2-pro-mistral-7b',
    description: 'Hermes 2 Pro on Mistral 7B is the new flagship 7B Hermes! Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.'
  },
  {
    name: 'bge-base-en',
    capability: 'embeddings',
    provider: 'cloudflare',
    providerModel: '@cf/baai/bge-base-en-v1.5',
    description: 'BAAI general embedding (Base) model that transforms any given text into a 768-dimensional vector'
  },
  {
    name: 'llama-3.1-8b-instruct-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.1-8b-instruct-awq',
    description: 'Quantized (int4) generative text model with 8 billion parameters from Meta. '
  },
  {
    name: 'uform-gen2-qwen-500m',
    capability: 'vision',
    provider: 'cloudflare',
    providerModel: '@cf/unum/uform-gen2-qwen-500m',
    description: 'UForm-Gen is a small generative vision-language model primarily designed for Image Captioning and Visual Question Answering. The model was pre-trained on the internal image captioning dataset and fine-tuned on public instructions datasets: SVIT, LVIS, VQAs datasets.'
  },
  {
    name: 'zephyr-7b-beta-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/thebloke/zephyr-7b-beta-awq',
    description: 'Zephyr 7B Beta AWQ is an efficient, accurate and blazing-fast low-bit weight quantized Zephyr model variant.'
  },
  {
    name: 'gemma-7b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/google/gemma-7b-it-lora',
    description: '  This is a Gemma-7B base model that Cloudflare dedicates for inference with LoRA adapters. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.'
  },
  {
    name: 'qwen-1.5-1.8b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/qwen/qwen1.5-1.8b-chat',
    description: 'Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud.'
  },
  {
    name: 'mistral-small-3.1',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/mistralai/mistral-small-3.1-24b-instruct',
    description: 'Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.'
  },
  {
    name: 'llama-3-8b-instruct-awq',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3-8b-instruct-awq',
    description: 'Quantized (int4) generative text model with 8 billion parameters from Meta.'
  },
  {
    name: 'llama-3.2-11b-vision',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.2-11b-vision-instruct',
    description: ' The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image.'
  },
  {
    name: 'whisper-tiny',
    capability: 'audio',
    provider: 'cloudflare',
    providerModel: '@cf/openai/whisper-tiny-en',
    description: 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. This is the English-only version of the Whisper Tiny model which was trained on the task of speech recognition.'
  },
  {
    name: 'whisper-large-v3-turbo',
    capability: 'audio',
    provider: 'cloudflare',
    providerModel: '@cf/openai/whisper-large-v3-turbo',
    description: 'Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. '
  },
  {
    name: 'aura-1',
    capability: 'tts',
    provider: 'cloudflare',
    providerModel: '@cf/deepgram/aura-1',
    description: 'Aura is a context-aware text-to-speech (TTS) model that applies natural pacing, expressiveness, and fillers based on the context of the provided text. The quality of your text input directly impacts the naturalness of the audio output.'
  },
  {
    name: 'sqlcoder-7b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/defog/sqlcoder-7b-2',
    description: 'This model is intended to be used by non-technical users to understand data inside their SQL databases. '
  },
  {
    name: 'phi-2',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/microsoft/phi-2',
    description: 'Phi-2 is a Transformer-based model with a next-word prediction objective, trained on 1.4T tokens from multiple passes on a mixture of Synthetic and Web datasets for NLP and coding.'
  },
  {
    name: 'meta-llama-3-8b-instruct',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/meta-llama/meta-llama-3-8b-instruct',
    description: 'Generation over generation, Meta Llama 3 demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning.	'
  },
  {
    name: 'bart-large-cnn',
    capability: 'summarization',
    provider: 'cloudflare',
    providerModel: '@cf/facebook/bart-large-cnn',
    description: 'BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. You can use this model for text summarization.'
  },
  {
    name: 'stable-diffusion-v1-5-img2img',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/runwayml/stable-diffusion-v1-5-img2img',
    description: 'Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images. Img2img generate a new image from an input image with Stable Diffusion. '
  },
  {
    name: 'gpt-oss-20b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/openai/gpt-oss-20b',
    description: 'OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases – gpt-oss-20b is for lower latency, and local or specialized use-cases.'
  },
  {
    name: 'embeddinggemma-300m',
    capability: 'embeddings',
    provider: 'cloudflare',
    providerModel: '@cf/google/embeddinggemma-300m',
    description: 'EmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.'
  },
  {
    name: 'bge-reranker-base',
    capability: 'text-classification',
    provider: 'cloudflare',
    providerModel: '@cf/baai/bge-reranker-base',
    description: 'Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. You can get a relevance score by inputting query and passage to the reranker. And the score can be mapped to a float value in [0,1] by sigmoid function.  '
  },
  {
    name: 'gemma-7b-it',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@hf/google/gemma-7b-it',
    description: 'Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants.'
  },
  {
    name: 'lucid-origin',
    capability: 'image-generation',
    provider: 'cloudflare',
    providerModel: '@cf/leonardo/lucid-origin',
    description: 'Lucid Origin from Leonardo.AI is their most adaptable and prompt-responsive model to date. Whether you\'re generating images with sharp graphic design, stunning full-HD renders, or highly specific creative direction, it adheres closely to your prompts, renders text with accuracy, and supports a wide array of visual styles and aesthetics – from stylized concept art to crisp product mockups. '
  },
  {
    name: 'qwen-1.5-14b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/qwen/qwen1.5-14b-chat-awq',
    description: 'Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud. AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization.'
  },
  {
    name: 'openchat-3.5',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/openchat/openchat-3.5-0106',
    description: 'OpenChat is an innovative library of open-source language models, fine-tuned with C-RLFT - a strategy inspired by offline reinforcement learning.'
  },
  {
    name: 'llama-4-scout-17b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-4-scout-17b-16e-instruct',
    description: 'Meta\'s Llama 4 Scout is a 17 billion parameter model with 16 experts that is natively multimodal. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.'
  },
  {
    name: 'gemma-3-12b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/google/gemma-3-12b-it',
    description: 'Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Gemma 3 models are multimodal, handling text and image input and generating text output, with a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions.'
  },
  {
    name: 'qwen-qwq-32b',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/qwen/qwq-32b',
    description: 'QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.'
  },
  {
    name: 'bge-large-en',
    capability: 'embeddings',
    provider: 'cloudflare',
    providerModel: '@cf/baai/bge-large-en-v1.5',
    description: 'BAAI general embedding (Large) model that transforms any given text into a 1024-dimensional vector'
  },
  {
    name: 'llama-3.1-70b-instruct',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.1-70b-instruct',
    description: 'Llama 3.1 70B Instruct'
  },
  {
    name: 'llama-3.1-8b-instruct-fast',
    capability: 'chat',
    provider: 'cloudflare',
    providerModel: '@cf/meta/llama-3.1-8b-instruct-fast',
    description: 'Llama 3.1 8B Instruct Fast'
  },
  {
    name: 'llama-3.3-70b',
    capability: 'chat',
    provider: 'external',
    providerModel: 'llama-3.3-70b',
    description: 'External model routed through model-router: llama-3.3-70b'
  },
  {
    name: 'llama-4-maverick-17b',
    capability: 'chat',
    provider: 'external',
    providerModel: 'llama-4-maverick-17b',
    description: 'External model routed through model-router: llama-4-maverick-17b'
  },
  {
    name: 'llama-3.1-8b-external',
    capability: 'chat',
    provider: 'external',
    providerModel: 'llama-3.1-8b-instruct',
    description: 'External model routed through model-router: llama-3.1-8b-instruct'
  },
  {
    name: 'deepseek-r1',
    capability: 'chat',
    provider: 'external',
    providerModel: 'deepseek-r1',
    description: 'External model routed through model-router: deepseek-r1'
  },
  {
    name: 'deepseek-v3-0324',
    capability: 'chat',
    provider: 'external',
    providerModel: 'deepseek-v3-0324',
    description: 'External model routed through model-router: deepseek-v3-0324'
  },
  {
    name: 'deepseek-r1-distill-llama-70b',
    capability: 'chat',
    provider: 'external',
    providerModel: 'deepseek-r1-distill-llama-70b',
    description: 'External model routed through model-router: deepseek-r1-distill-llama-70b'
  },
  {
    name: 'qwen-3-32b',
    capability: 'chat',
    provider: 'external',
    providerModel: 'qwen-3-32b',
    description: 'External model routed through model-router: qwen-3-32b'
  },
  {
    name: 'llama-3.3-swallow-70b',
    capability: 'chat',
    provider: 'external',
    providerModel: 'llama-3.3-swallow-70b',
    description: 'External model routed through model-router: llama-3.3-swallow-70b'
  },
  {
    name: 'whisper-large-v3',
    capability: 'audio',
    provider: 'external',
    providerModel: 'whisper-large-v3',
    description: 'External model routed through model-router: whisper-large-v3'
  },
  {
    name: 'llama-3.1-8b-instant',
    capability: 'chat',
    provider: 'external',
    providerModel: 'llama-3.1-8b-instant',
    description: 'External model routed through model-router: llama-3.1-8b-instant'
  },
  {
    name: 'gemma-9b-it',
    capability: 'chat',
    provider: 'external',
    providerModel: 'gemma-9b-it',
    description: 'External model routed through model-router: gemma-9b-it'
  },
  {
    name: 'kimi-k2',
    capability: 'chat',
    provider: 'external',
    providerModel: 'kimi-k2',
    description: 'External model routed through model-router: kimi-k2'
  },
  {
    name: 'gpt-oss-120b-test',
    capability: 'chat',
    provider: 'external',
    providerModel: 'gpt-oss-120b-test',
    description: 'External model routed through model-router: gpt-oss-120b-test'
  },
  {
    name: 'embeddings',
    capability: 'embeddings',
    provider: 'external',
    providerModel: 'bge-embeddings',
    description: 'External model routed through model-router: bge-embeddings'
  },
  {
    name: 'pii-detection',
    capability: 'pii-detection',
    provider: 'external',
    providerModel: 'piiranha-pii',
    description: 'External model routed through model-router: piiranha-pii'
  }
] as const;

/** Available model names */
export type AvailableModel = typeof MODEL_CATALOG[number]['name'];

/** Model capabilities */
export type ModelCapability = 'chat' | 'embeddings' | 'audio' | 'tts' | 'pii-detection' | 'image-generation' | 'text-classification' | 'image-classification' | 'translation' | 'summarization' | 'vision';

/** Model providers */
export type ModelProvider = 'cloudflare' | 'external';

/** Model catalog entry */
export interface ModelCatalogEntry {
  name: string;
  capability: ModelCapability;
  provider: ModelProvider;
  providerModel: string;
  description: string;
}

/** Get all models by capability */
export function getModelsByCapability(capability: ModelCapability): ModelCatalogEntry[] {
  return MODEL_CATALOG.filter(model => model.capability === capability);
}

/** Get model information by name */
export function getModelInfo(modelName: AvailableModel): ModelCatalogEntry | undefined {
  return MODEL_CATALOG.find(model => model.name === modelName);
}

/** OpenAI-compatible chat input interface */
export interface OpenAIChatInput {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  model?: string;
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
  response_format?: {
    type: 'json_object' | 'text';
  };
}

/** OpenAI-compatible chat input interface for external models (requires model field) */
export interface OpenAIChatInputExternal extends Omit<OpenAIChatInput, 'model'> {
  model: string; // Required for external model routing
}

/** OpenAI-compatible chat output interface */
export interface OpenAIChatOutput {
  choices: Array<{
    message: {
      role: 'assistant';
      content: string;
    };
    finish_reason?: string;
  }>;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

/** OpenAI-compatible embedding output interface */
export interface OpenAIEmbeddingOutput {
  data: Array<{
    embedding: number[];
    index: number;
  }>;
  model?: string;
  usage?: {
    prompt_tokens: number;
    total_tokens: number;
  };
}

/** OpenAI-compatible audio output interface */
export interface OpenAIAudioOutput {
  text: string;
}

/** Image generation input interface */
export interface ImageGenerationInput {
  prompt: string;
  model?: string;
  n?: number;
  size?: string;
  quality?: string;
  style?: string;
  response_format?: 'url' | 'b64_json';
}

/** Image generation output interface */
export interface ImageGenerationOutput {
  data: Array<{
    url?: string;
    b64_json?: string;
  }>;
}

/** Text classification input interface */
export interface TextClassificationInput {
  text: string;
  model?: string;
}

/** Text classification output interface */
export interface TextClassificationOutput {
  label: string;
  score: number;
}

/** Image classification input interface */
export interface ImageClassificationInput {
  image: File | Blob | string;
  model?: string;
}

/** Image classification output interface */
export interface ImageClassificationOutput {
  label: string;
  score: number;
}

/** Translation input interface */
export interface TranslationInput {
  text: string;
  source_lang?: string;
  target_lang: string;
  model?: string;
}

/** Translation output interface */
export interface TranslationOutput {
  translation: string;
  source_lang?: string;
  target_lang: string;
}

/** Summarization input interface */
export interface SummarizationInput {
  text: string;
  max_length?: number;
  min_length?: number;
  model?: string;
}

/** Summarization output interface */
export interface SummarizationOutput {
  summary: string;
}

/** Vision input interface */
export interface VisionInput {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: Array<{
      type: 'text' | 'image_url';
      text?: string;
      image_url?: {
        url: string; // Can be URL or data:image/... URI
        detail?: 'low' | 'high' | 'auto';
      };
    }>;
  }>;
  model: string; // Required to make it compatible with OpenAIChatInputExternal
  max_tokens?: number;
  temperature?: number;
  stream?: boolean;
}

/** Vision output interface */
export interface VisionOutput {
  choices: Array<{
    message: {
      role: 'assistant';
      content: string;
    };
    finish_reason?: string;
  }>;
}

/** Configuration options for AI gateway requests */
export type GatewayOptions = {
  /** Unique identifier for the request */
  id: string;
  /** Cache key for the request */
  cacheKey?: string;
  /** Time-to-live in seconds for cache entries */
  cacheTtl?: number;
  /** Whether to bypass cache for this request */
  skipCache?: boolean;
  /** Additional metadata to attach to the request */
  metadata?: Record<string, number | string | boolean | null | bigint>;
  /** Whether to collect logs for this request */
  collectLog?: boolean;
  /** Event ID for tracking */
  eventId?: string;
  /** Request timeout in milliseconds */
  requestTimeoutMs?: number;
};

/** Retry configuration for gateway requests */
export type GatewayRetries = {
  /** Maximum number of retries */
  maxRetries?: number;
  /** Backoff strategy */
  backoffStrategy?: 'exponential' | 'linear' | 'fixed';
  /** Initial delay in milliseconds */
  initialDelay?: number;
  /** Maximum delay in milliseconds */
  maxDelay?: number;
};

/** General options for AI operations */
export type AiOptions = {
  /** Process the request asynchronously as a batch */
  queueRequest?: boolean;
  /** Return the raw Response object instead of parsed data */
  returnRawResponse?: boolean;
  /** Gateway-specific configuration */
  gateway?: GatewayOptions;
  /** URL prefix for API endpoints */
  prefix?: string;
  /** Additional HTTP headers to include */
  extraHeaders?: Record<string, string>;
};

/** Extended options for AI operations including smart bucket authentication */
export interface ExtendedAiOptions extends AiOptions {
  includeTimingData?: boolean;
  stream?: boolean;
  
  smartBucketAuth?: {
    bucketId: string;
    secret: string;
  };
};

// ========================================
// STANDARDIZED PUBLIC INTERFACES
// ========================================

/** Reranker models (bge-reranker-base, etc.) */
export interface RerankerInput {
  query: string;
  documents: string[];
  top_k?: number;
}

/** Summarization models (bart-large-cnn, etc.) */
export interface SummarizationInput {
  text: string;
  max_length?: number;
  min_length?: number;
}

/** Translation models (m2m100-1.2b, etc.) */
export interface TranslationInput {
  text: string;
  source_language?: string;
  target_language: string;
}

/** Text Classification models (distilbert-sst-2-int8, etc.) */
export interface TextClassificationInput {
  text: string;
  labels?: string[];
}

/** Image Generation models (flux-1-schnell, stable-diffusion, etc.) */
export interface ImageGenerationInput {
  prompt: string;
  negative_prompt?: string;
  width?: number;
  height?: number;
  steps?: number;
  guidance_scale?: number;
}

/** Image Classification models (resnet-50, etc.) */
export interface ImageClassificationInput {
  image: string | File | Blob; // URL, File, or Blob
  prompt?: string;
}

/** Vision models (llava-1.5-7b, etc.) */
export interface VisionInput {
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: Array<{
      type: 'text' | 'image_url';
      text?: string;
      image_url?: {
        url: string; // Can be URL or data:image/... URI
        detail?: 'low' | 'high' | 'auto';
      };
    }>;
  }>;
  model: string; // Required to make it compatible with OpenAIChatInputExternal
  max_tokens?: number;
  temperature?: number;
  stream?: boolean;
}

/** TTS models (aura-1, melotts, etc.) */
export interface TTSInput {
  text: string;
  voice?: string;
  speed?: number;
  response_format?: 'mp3' | 'wav' | 'ogg';
}

/** Audio models (whisper variants, etc.) - Supports both array format for RPC serialization and ReadableStream for large files */
export interface AudioInput {
  audio: number[] | ReadableStream<Uint8Array>; // Audio data as number array for RPC serialization or ReadableStream for large files
  contentType: string; // MIME type (e.g., 'audio/mpeg', 'audio/wav')
  language?: string;
  prompt?: string;
  response_format?: 'json' | 'text' | 'srt' | 'vtt';
}

// ========================================
// STANDARDIZED OUTPUT INTERFACES
// ========================================

/** Reranker output interface */
export interface RerankerOutput {
  ranked_documents: Array<{
    index: number;
    document: string;
    relevance_score: number;
  }>;
}

/** Summarization output interface */
export interface SummarizationOutput {
  summary: string;
}

/** Translation output interface */
export interface TranslationOutput {
  translation: string;
  source_lang?: string;
  target_lang: string;
}

/** Text classification output interface */
export interface TextClassificationOutput {
  label: string;
  score: number;
}

/** Image generation output interface */
export interface ImageGenerationOutput {
  data: Array<{
    url?: string;
    b64_json?: string;
  }>;
}

/** Image classification output interface */
export interface ImageClassificationOutput {
  label: string;
  score: number;
}

/** Vision output interface */
export interface VisionOutput {
  choices: Array<{
    message: {
      role: 'assistant';
      content: string;
    };
    finish_reason?: string;
  }>;
}

/** TTS output interface */
export interface TTSOutput {
  audio: ArrayBuffer | Uint8Array;
  response_format?: string;
}

/** Audio output interface */
export interface AudioOutput {
  text: string;
}

export type AiInput = ({
/**
 * readable stream with audio data and content-type specified for that data
 */
audio: {
body: {

}
contentType: string
}
/**
 * type of data PCM data that's sent to the inference server as raw array
 */
dtype?: ("uint8" | "float32" | "float64")
} | {
/**
 * base64 encoded audio data
 */
audio: string
/**
 * type of data PCM data that's sent to the inference server as raw array
 */
dtype?: ("uint8" | "float32" | "float64")
})

export interface AiOutput {
/**
 * if true, end-of-turn was detected
 */
is_complete?: boolean
/**
 * probability of the end-of-turn detection
 */
probability?: number
}

export type AiInput2 = (Prompt | Messages)

export interface Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
 */
lora?: string
response_format?: JSONMode
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface JSONMode {
type?: ("json_object" | "json_schema")
json_schema?: any
}
export interface Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role: string
/**
 * The content of the message as a string.
 */
content: string
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
response_format?: JSONMode1
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface JSONMode1 {
type?: ("json_object" | "json_schema")
json_schema?: any
}

export interface AiOutput2 {
/**
 * The generated text response from the model
 */
response: string
/**
 * Usage statistics for the inference request
 */
usage?: {
/**
 * Total number of tokens in input
 */
prompt_tokens?: number
/**
 * Total number of tokens in output
 */
completion_tokens?: number
/**
 * Total number of input and output tokens
 */
total_tokens?: number
}
/**
 * An array of tool calls requests made during the response generation
 */
tool_calls?: {
/**
 * The arguments passed to be passed to the tool call request
 */
arguments?: {

}
/**
 * The name of the tool to be called
 */
name?: string
}[]
}

export type AiInput3 = (BGEM3InputQueryAndContexts | BGEM3InputEmbedding | {
/**
 * Batch of the embeddings requests to run using async-queue
 */
requests: (BGEM3InputQueryAndContexts1 | BGEM3InputEmbedding1)[]
})

export interface BGEM3InputQueryAndContexts {
/**
 * A query you wish to perform against the provided contexts. If no query is provided the model with respond with embeddings for contexts
 */
query?: string
/**
 * List of provided contexts. Note that the index in this array is important, as the response will refer to it.
 */
contexts: {
/**
 * One of the provided context content
 */
text?: string
}[]
/**
 * When provided with too long context should the model error out or truncate the context to fit?
 */
truncate_inputs?: boolean
}
export interface BGEM3InputEmbedding {
text: (string | string[])
/**
 * When provided with too long context should the model error out or truncate the context to fit?
 */
truncate_inputs?: boolean
}
export interface BGEM3InputQueryAndContexts1 {
/**
 * A query you wish to perform against the provided contexts. If no query is provided the model with respond with embeddings for contexts
 */
query?: string
/**
 * List of provided contexts. Note that the index in this array is important, as the response will refer to it.
 */
contexts: {
/**
 * One of the provided context content
 */
text?: string
}[]
/**
 * When provided with too long context should the model error out or truncate the context to fit?
 */
truncate_inputs?: boolean
}
export interface BGEM3InputEmbedding1 {
text: (string | string[])
/**
 * When provided with too long context should the model error out or truncate the context to fit?
 */
truncate_inputs?: boolean
}

// Note: This type excludes AsyncResponse variants which are handled by the queueRequest option
export type AiOutput3 = (BGEM3OuputQuery | BGEM3OutputEmbeddingForContexts | BGEM3OuputEmbedding)

export interface BGEM3OuputQuery {
response?: {
/**
 * Index of the context in the request
 */
id?: number
/**
 * Score of the context under the index.
 */
score?: number
}[]
}
export interface BGEM3OutputEmbeddingForContexts {
response?: number[][]
shape?: number[]
/**
 * The pooling method used in the embedding process.
 */
pooling?: ("mean" | "cls")
}
export interface BGEM3OuputEmbedding {
shape?: number[]
/**
 * Embeddings of the requested text values
 */
data?: number[][]
/**
 * The pooling method used in the embedding process.
 */
pooling?: ("mean" | "cls")
}

export interface AiInput4 {
/**
 * The text that you want to classify
 */
text: string
}

/**
 * An array of classification results for the input text
 */
export type AiOutput4 = {
/**
 * Confidence score indicating the likelihood that the text belongs to the specified label
 */
score?: number
/**
 * The classification label assigned to the text (e.g., 'POSITIVE' or 'NEGATIVE')
 */
label?: string
}[]

export interface AiInput5 {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender must alternate between 'user' and 'assistant'.
 */
role: ("user" | "assistant")
/**
 * The content of the message as a string.
 */
content: string
}[]
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Dictate the output format of the generated response.
 */
response_format?: {
/**
 * Set to json_object to process and output generated text as JSON.
 */
type?: string
}
}

export interface AiOutput5 {
response?: (string | {
/**
 * Whether the conversation is safe or not.
 */
safe?: boolean
/**
 * A list of what hazard categories predicted for the conversation, if the conversation is deemed unsafe.
 */
categories?: string[]
})
/**
 * Usage statistics for the inference request
 */
usage?: {
/**
 * Total number of tokens in input
 */
prompt_tokens?: number
/**
 * Total number of tokens in output
 */
completion_tokens?: number
/**
 * Total number of input and output tokens
 */
total_tokens?: number
}
}

export interface AiInput6 {
/**
 * A text description of the audio you want to generate
 */
prompt: string
/**
 * The speech language (e.g., 'en' for English, 'fr' for French). Defaults to 'en' if not specified
 */
lang?: string
}

export interface AiOutput6 {
/**
 * The generated audio in MP3 format, base64-encoded
 */
audio?: string
}

export type AiInput7 = (string | {
/**
 * An array of integers that represent the audio data constrained to 8-bit unsigned integer values
 */
audio: number[]
})

export interface AiOutput7 {
/**
 * The transcription
 */
text: string
word_count?: number
words?: {
word?: string
/**
 * The second this word begins in the recording
 */
start?: number
/**
 * The ending second when the word completes
 */
end?: number
}[]
vtt?: string
}

export type AiInput8 = (string | {
image: (number[] | string)
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * The input text prompt for the model to generate a response.
 */
prompt?: string
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
})

export interface AiOutput8 {
description?: string
}

export interface AiInput9 {
/**
 * A text description of the image you want to generate
 */
prompt: string
/**
 * Text describing elements to avoid in the generated image
 */
negative_prompt?: string
/**
 * The height of the generated image in pixels
 */
height?: number
/**
 * The width of the generated image in pixels
 */
width?: number
/**
 * For use with img2img tasks. An array of integers that represent the image data constrained to 8-bit unsigned integer values
 */
image?: number[]
/**
 * For use with img2img tasks. A base64-encoded string of the input image
 */
image_b64?: string
/**
 * An array representing An array of integers that represent mask image data for inpainting constrained to 8-bit unsigned integer values
 */
mask?: number[]
/**
 * The number of diffusion steps; higher values can improve quality but take longer
 */
num_steps?: number
/**
 * A value between 0 and 1 indicating how strongly to apply the transformation during img2img tasks; lower values make the output closer to the input image
 */
strength?: number
/**
 * Controls how closely the generated image should adhere to the prompt; higher values make the image more aligned with the prompt
 */
guidance?: number
/**
 * Random seed for reproducibility of the image generation
 */
seed?: number
}

/**
 * The generated image in PNG format
 */
export type AiOutput9 = string

export interface AiInput10 {
audio: {
body: {

}
contentType: string
}
/**
 * Sets how the model will interpret strings submitted to the custom_topic param. When strict, the model will only return topics submitted using the custom_topic param. When extended, the model will return its own detected topics in addition to those submitted using the custom_topic param.
 */
custom_topic_mode?: ("extended" | "strict")
/**
 * Custom topics you want the model to detect within your input audio or text if present Submit up to 100
 */
custom_topic?: string
/**
 * Sets how the model will interpret intents submitted to the custom_intent param. When strict, the model will only return intents submitted using the custom_intent param. When extended, the model will return its own detected intents in addition those submitted using the custom_intents param
 */
custom_intent_mode?: ("extended" | "strict")
/**
 * Custom intents you want the model to detect within your input audio if present
 */
custom_intent?: string
/**
 * Identifies and extracts key entities from content in submitted audio
 */
detect_entities?: boolean
/**
 * Identifies the dominant language spoken in submitted audio
 */
detect_language?: boolean
/**
 * Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0
 */
diarize?: boolean
/**
 * Identify and extract key entities from content in submitted audio
 */
dictation?: boolean
/**
 * Specify the expected encoding of your submitted audio
 */
encoding?: ("linear16" | "flac" | "mulaw" | "amr-nb" | "amr-wb" | "opus" | "speex" | "g729")
/**
 * Arbitrary key-value pairs that are attached to the API response for usage in downstream processing
 */
extra?: string
/**
 * Filler Words can help transcribe interruptions in your audio, like 'uh' and 'um'
 */
filler_words?: boolean
/**
 * Key term prompting can boost or suppress specialized terminology and brands.
 */
keyterm?: string
/**
 * Keywords can boost or suppress specialized terminology and brands.
 */
keywords?: string
/**
 * The BCP-47 language tag that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available.
 */
language?: string
/**
 * Spoken measurements will be converted to their corresponding abbreviations.
 */
measurements?: boolean
/**
 * Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip.
 */
mip_opt_out?: boolean
/**
 * Mode of operation for the model representing broad area of topic that will be talked about in the supplied audio
 */
mode?: ("general" | "medical" | "finance")
/**
 * Transcribe each audio channel independently.
 */
multichannel?: boolean
/**
 * Numerals converts numbers from written format to numerical format.
 */
numerals?: boolean
/**
 * Splits audio into paragraphs to improve transcript readability.
 */
paragraphs?: boolean
/**
 * Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely.
 */
profanity_filter?: boolean
/**
 * Add punctuation and capitalization to the transcript.
 */
punctuate?: boolean
/**
 * Redaction removes sensitive information from your transcripts.
 */
redact?: string
/**
 * Search for terms or phrases in submitted audio and replaces them.
 */
replace?: string
/**
 * Search for terms or phrases in submitted audio.
 */
search?: string
/**
 * Recognizes the sentiment throughout a transcript or text.
 */
sentiment?: boolean
/**
 * Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability.
 */
smart_format?: boolean
/**
 * Detect topics throughout a transcript or text.
 */
topics?: boolean
/**
 * Segments speech into meaningful semantic units.
 */
utterances?: boolean
/**
 * Seconds to wait before detecting a pause between words in submitted audio.
 */
utt_split?: number
/**
 * The number of channels in the submitted audio
 */
channels?: number
/**
 * Specifies whether the streaming endpoint should provide ongoing transcription updates as more audio is received. When set to true, the endpoint sends continuous updates, meaning transcription results may evolve over time. Note: Supported only for webosockets.
 */
interim_results?: boolean
/**
 * Indicates how long model will wait to detect whether a speaker has finished speaking or pauses for a significant period of time. When set to a value, the streaming endpoint immediately finalizes the transcription for the processed time range and returns the transcript with a speech_final parameter set to true. Can also be set to false to disable endpointing
 */
endpointing?: string
/**
 * Indicates that speech has started. You'll begin receiving Speech Started messages upon speech starting. Note: Supported only for webosockets.
 */
vad_events?: boolean
/**
 * Indicates how long model will wait to send an UtteranceEnd message after a word has been transcribed. Use with interim_results. Note: Supported only for webosockets.
 */
utterance_end_ms?: boolean
}

export interface AiOutput10 {
results?: {
channels?: {
alternatives?: {
confidence?: number
transcript?: string
words?: {
confidence?: number
end?: number
start?: number
word?: string
}[]
}[]
}[]
summary?: {
result?: string
short?: string
}
sentiments?: {
segments?: {
text?: string
start_word?: number
end_word?: number
sentiment?: string
sentiment_score?: number
}[]
average?: {
sentiment?: string
sentiment_score?: number
}
}
}
}

export interface AiInput11 {
/**
 * A text description of the image you want to generate.
 */
prompt: string
/**
 * The number of diffusion steps; higher values can improve quality but take longer.
 */
steps?: number
}

export interface AiOutput11 {
/**
 * The generated image in Base64 format.
 */
image?: string
}

export type AiInput12 = (string | {
/**
 * An array of integers that represent the image data constrained to 8-bit unsigned integer values
 */
image: number[]
})

export type AiOutput12 = {
/**
 * A confidence value, between 0 and 1, indicating how certain the model is about the predicted label
 */
score?: number
/**
 * The predicted category or class for the input image based on analysis
 */
label?: string
}[]

export type AiInput13 = (Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Prompt | Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Messages | AsyncBatch)

export interface Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
 */
lora?: string
response_format?: JSONMode
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface _cf_meta_llama_3_3_70b_instruct_fp8_fast_AiInput13_JSONMode {
type?: ("json_object" | "json_schema")
json_schema?: any
}
export interface Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role: string
/**
 * The content of the message as a string.
 */
content: string
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
response_format?: JSONMode1
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface _cf_meta_llama_3_3_70b_instruct_fp8_fast_AiInput13_JSONMode1 {
type?: ("json_object" | "json_schema")
json_schema?: any
}
export interface AsyncBatch {
requests?: {
/**
 * User-supplied reference. This field will be present in the response as well it can be used to reference the request and response. It's NOT validated to be unique.
 */
external_reference?: string
/**
 * Prompt for the text generation model
 */
prompt?: string
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
response_format?: JSONMode2
}[]
}
export interface JSONMode2 {
type?: ("json_object" | "json_schema")
json_schema?: any
}

// Note: This type excludes AsyncResponse variants which are handled by the queueRequest option
export interface AiOutput13 {
/**
 * The generated text response from the model
 */
response: string
/**
 * Usage statistics for the inference request
 */
usage?: {
/**
 * Total number of tokens in input
 */
prompt_tokens?: number
/**
 * Total number of tokens in output
 */
completion_tokens?: number
/**
 * Total number of input and output tokens
 */
total_tokens?: number
}
/**
 * An array of tool calls requests made during the response generation
 */
tool_calls?: {
/**
 * The arguments passed to be passed to the tool call request
 */
arguments?: {

}
/**
 * The name of the tool to be called
 */
name?: string
}[]
}

export interface AiInput14 {
/**
 * A text description of the image you want to generate.
 */
prompt: string
/**
 * Controls how closely the generated image should adhere to the prompt; higher values make the image more aligned with the prompt
 */
guidance?: number
/**
 * Random seed for reproducibility of the image generation
 */
seed?: number
/**
 * The height of the generated image in pixels
 */
height?: number
/**
 * The width of the generated image in pixels
 */
width?: number
/**
 * The number of diffusion steps; higher values can improve quality but take longer
 */
num_steps?: number
/**
 * Specify what to exclude from the generated images
 */
negative_prompt?: string
}

/**
 * The generated image in JPEG format
 */
export type AiOutput14 = string

export type AiInput15 = ({
/**
 * The text to be translated
 */
text: string
/**
 * The language code of the source text (e.g., 'en' for English). Defaults to 'en' if not specified
 */
source_lang?: string
/**
 * The language code to translate the text into (e.g., 'es' for Spanish)
 */
target_lang: string
} | {
/**
 * Batch of the embeddings requests to run using async-queue
 */
requests: {
/**
 * The text to be translated
 */
text: string
/**
 * The language code of the source text (e.g., 'en' for English). Defaults to 'en' if not specified
 */
source_lang?: string
/**
 * The language code to translate the text into (e.g., 'es' for Spanish)
 */
target_lang: string
}[]
})

// Note: This type excludes AsyncResponse variants which are handled by the queueRequest option
export interface AiOutput15 {
/**
 * The translated text in the target language
 */
translated_text?: string
}

export type AiInput16 = ({
text: (string | string[])
/**
 * The pooling method used in the embedding process. `cls` pooling will generate more accurate embeddings on larger inputs - however, embeddings created with cls pooling are not compatible with embeddings generated with mean pooling. The default pooling method is `mean` in order for this to not be a breaking change, but we highly suggest using the new `cls` pooling for better accuracy.
 */
pooling?: ("mean" | "cls")
} | {
/**
 * Batch of the embeddings requests to run using async-queue
 */
requests: {
text: (string | string[])
/**
 * The pooling method used in the embedding process. `cls` pooling will generate more accurate embeddings on larger inputs - however, embeddings created with cls pooling are not compatible with embeddings generated with mean pooling. The default pooling method is `mean` in order for this to not be a breaking change, but we highly suggest using the new `cls` pooling for better accuracy.
 */
pooling?: ("mean" | "cls")
}[]
})

// Note: This type excludes AsyncResponse variants which are handled by the queueRequest option
export interface AiOutput16 {
shape?: number[]
/**
 * Embeddings of the requested text values
 */
data?: number[][]
/**
 * The pooling method used in the embedding process.
 */
pooling?: ("mean" | "cls")
}

export type AiInput17 = (Qwen2_5_Coder_32B_Instruct_Prompt | Qwen2_5_Coder_32B_Instruct_Messages)

export interface Qwen2_5_Coder_32B_Instruct_Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
 */
lora?: string
response_format?: JSONMode
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface _cf_qwen_qwen2_5_coder_32b_instruct_AiInput17_JSONMode {
type?: ("json_object" | "json_schema")
json_schema?: any
}
export interface Qwen2_5_Coder_32B_Instruct_Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role: string
/**
 * The content of the message as a string.
 */
content: string
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
response_format?: JSONMode1
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface _cf_qwen_qwen2_5_coder_32b_instruct_AiInput17_JSONMode1 {
type?: ("json_object" | "json_schema")
json_schema?: any
}

export type AiInput18 = (string | {
/**
 * The input text prompt for the model to generate a response.
 */
prompt?: string
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
image: (number[] | string)
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
})

export type AiInput19 = (Mistral_Small_3_1_24B_Instruct_Prompt | Mistral_Small_3_1_24B_Instruct_Messages)

export interface Mistral_Small_3_1_24B_Instruct_Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * JSON schema that should be fulfilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface Mistral_Small_3_1_24B_Instruct_Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role?: string
/**
 * The tool call id. Must be supplied for tool calls for Mistral-3. If you don't know what to put here you can fall back to 000000001
 */
tool_call_id?: string
content?: (string | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
}[] | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
})
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
/**
 * JSON schema that should be fufilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}

export interface AiOutput17 {
/**
 * The generated text response from the model
 */
response: string
/**
 * Usage statistics for the inference request
 */
usage?: {
/**
 * Total number of tokens in input
 */
prompt_tokens?: number
/**
 * Total number of tokens in output
 */
completion_tokens?: number
/**
 * Total number of input and output tokens
 */
total_tokens?: number
}
/**
 * An array of tool calls requests made during the response generation
 */
tool_calls?: {
/**
 * The arguments passed to be passed to the tool call request
 */
arguments?: {

}
/**
 * The name of the tool to be called
 */
name?: string
}[]
}

export type AiInput20 = (_cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Prompt | _cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Messages)

export interface _cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
image?: (number[] | string)
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
/**
 * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
 */
lora?: string
}
export interface _cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role?: string
/**
 * The tool call id. Must be supplied for tool calls for Mistral-3. If you don't know what to put here you can fall back to 000000001
 */
tool_call_id?: string
content?: (string | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
}[] | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
})
}[]
image?: (number[] | string)
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
/**
 * If true, the response will be streamed back incrementally.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}

export interface AiOutput18 {
/**
 * The generated text response from the model
 */
response?: string
/**
 * An array of tool calls requests made during the response generation
 */
tool_calls?: {
/**
 * The arguments passed to be passed to the tool call request
 */
arguments?: {

}
/**
 * The name of the tool to be called
 */
name?: string
}[]
}

export interface AiInput21 {
/**
 * Base64 encoded value of the audio data.
 */
audio: string
/**
 * Supported tasks are 'translate' or 'transcribe'.
 */
task?: string
/**
 * The language of the audio being transcribed or translated.
 */
language?: string
/**
 * Preprocess the audio with a voice activity detection model.
 */
vad_filter?: boolean
/**
 * A text prompt to help provide context to the model on the contents of the audio.
 */
initial_prompt?: string
/**
 * The prefix it appended the the beginning of the output of the transcription and can guide the transcription result.
 */
prefix?: string
}

export interface AiOutput19 {
transcription_info?: {
/**
 * The language of the audio being transcribed or translated.
 */
language?: string
/**
 * The confidence level or probability of the detected language being accurate, represented as a decimal between 0 and 1.
 */
language_probability?: number
/**
 * The total duration of the original audio file, in seconds.
 */
duration?: number
/**
 * The duration of the audio after applying Voice Activity Detection (VAD) to remove silent or irrelevant sections, in seconds.
 */
duration_after_vad?: number
}
/**
 * The complete transcription of the audio.
 */
text: string
/**
 * The total number of words in the transcription.
 */
word_count?: number
segments?: {
/**
 * The starting time of the segment within the audio, in seconds.
 */
start?: number
/**
 * The ending time of the segment within the audio, in seconds.
 */
end?: number
/**
 * The transcription of the segment.
 */
text?: string
/**
 * The temperature used in the decoding process, controlling randomness in predictions. Lower values result in more deterministic outputs.
 */
temperature?: number
/**
 * The average log probability of the predictions for the words in this segment, indicating overall confidence.
 */
avg_logprob?: number
/**
 * The compression ratio of the input to the output, measuring how much the text was compressed during the transcription process.
 */
compression_ratio?: number
/**
 * The probability that the segment contains no speech, represented as a decimal between 0 and 1.
 */
no_speech_prob?: number
words?: {
/**
 * The individual word transcribed from the audio.
 */
word?: string
/**
 * The starting time of the word within the audio, in seconds.
 */
start?: number
/**
 * The ending time of the word within the audio, in seconds.
 */
end?: number
}[]
}[]
/**
 * The transcription in WebVTT format, which includes timing and text information for use in subtitles.
 */
vtt?: string
}

export interface AiInput22 {
/**
 * Speaker used to produce the audio.
 */
speaker?: ("angus" | "asteria" | "arcas" | "orion" | "orpheus" | "athena" | "luna" | "zeus" | "perseus" | "helios" | "hera" | "stella")
/**
 * Encoding of the output audio.
 */
encoding?: ("linear16" | "flac" | "mulaw" | "alaw" | "mp3" | "opus" | "aac")
/**
 * Container specifies the file format wrapper for the output audio. The available options depend on the encoding type..
 */
container?: ("none" | "wav" | "ogg")
/**
 * The text content to be converted to speech
 */
text: string
/**
 * Sample Rate specifies the sample rate for the output audio. Based on the encoding, different sample rates are supported. For some encodings, the sample rate is not configurable
 */
sample_rate?: number
/**
 * The bitrate of the audio in bits per second. Choose from predefined ranges or specific values based on the encoding type.
 */
bit_rate?: number
}

/**
 * The generated audio in MP3 format
 */
export type AiOutput20 = string

export interface AiInput23 {
/**
 * The text that you want the model to summarize
 */
input_text: string
/**
 * The maximum length of the generated summary in tokens
 */
max_length?: number
}

export interface AiOutput21 {
/**
 * The summarized version of the input text
 */
summary?: string
}

export interface AiInput24 {
text: (string | string[])
}

export interface AiOutput22 {
shape?: number[]
/**
 * Embeddings of the requested text values
 */
data?: number[][]
}

export interface AiInput25 {
/**
 * A query you wish to perform against the provided contexts.
 */
query: string
/**
 * Number of returned results starting with the best score.
 */
top_k?: number
/**
 * List of provided contexts. Note that the index in this array is important, as the response will refer to it.
 */
contexts: {
/**
 * One of the provided context content
 */
text?: string
}[]
}

export interface AiOutput23 {
response?: {
/**
 * Index of the context in the request
 */
id?: number
/**
 * Score of the context under the index.
 */
score?: number
}[]
}

export interface AiInput26 {
/**
 * A text description of the image you want to generate.
 */
prompt: string
/**
 * Controls how closely the generated image should adhere to the prompt; higher values make the image more aligned with the prompt
 */
guidance?: number
/**
 * Random seed for reproducibility of the image generation
 */
seed?: number
/**
 * The height of the generated image in pixels
 */
height?: number
/**
 * The width of the generated image in pixels
 */
width?: number
/**
 * The number of diffusion steps; higher values can improve quality but take longer
 */
num_steps?: number
/**
 * The number of diffusion steps; higher values can improve quality but take longer
 */
steps?: number
}

export type AiInput27 = (Ai_Cf_Meta_Llama_4_Prompt | Ai_Cf_Meta_Llama_4_Messages | Ai_Cf_Meta_Llama_4_Async_Batch)

export interface Ai_Cf_Meta_Llama_4_Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * JSON schema that should be fulfilled for the response.
 */
guided_json?: {

}
response_format?: JSONMode
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface _cf_meta_llama_4_scout_17b_16e_instruct_AiInput27_JSONMode {
type?: ("json_object" | "json_schema")
json_schema?: any
}
export interface Ai_Cf_Meta_Llama_4_Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role?: string
/**
 * The tool call id. If you don't know what to put here you can fall back to 000000001
 */
tool_call_id?: string
content?: (string | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
}[] | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
})
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
response_format?: JSONMode1
/**
 * JSON schema that should be fufilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface _cf_meta_llama_4_scout_17b_16e_instruct_AiInput27_JSONMode1 {
type?: ("json_object" | "json_schema")
json_schema?: any
}
export interface Ai_Cf_Meta_Llama_4_Async_Batch {
requests: (Ai_Cf_Meta_Llama_4_Prompt_Inner | Ai_Cf_Meta_Llama_4_Messages_Inner)[]
}
export interface Ai_Cf_Meta_Llama_4_Prompt_Inner {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * JSON schema that should be fulfilled for the response.
 */
guided_json?: {

}
response_format?: JSONMode2
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface _cf_meta_llama_4_scout_17b_16e_instruct_AiInput27_JSONMode2 {
type?: ("json_object" | "json_schema")
json_schema?: any
}
export interface Ai_Cf_Meta_Llama_4_Messages_Inner {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role?: string
/**
 * The tool call id. If you don't know what to put here you can fall back to 000000001
 */
tool_call_id?: string
content?: (string | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
}[] | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
})
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
response_format?: JSONMode3
/**
 * JSON schema that should be fufilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface JSONMode3 {
type?: ("json_object" | "json_schema")
json_schema?: any
}

export interface AiOutput24 {
/**
 * The generated text response from the model
 */
response: string
/**
 * Usage statistics for the inference request
 */
usage?: {
/**
 * Total number of tokens in input
 */
prompt_tokens?: number
/**
 * Total number of tokens in output
 */
completion_tokens?: number
/**
 * Total number of input and output tokens
 */
total_tokens?: number
}
/**
 * An array of tool calls requests made during the response generation
 */
tool_calls?: {
/**
 * The tool call id.
 */
id?: string
/**
 * Specifies the type of tool (e.g., 'function').
 */
type?: string
/**
 * Details of the function tool.
 */
function?: {
/**
 * The name of the tool to be called
 */
name?: string
/**
 * The arguments passed to be passed to the tool call request
 */
arguments?: {

}
}
}[]
}

export type AiInput28 = (Google_Gemma_3_12B_It_Prompt | Google_Gemma_3_12B_It_Messages)

export interface Google_Gemma_3_12B_It_Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * JSON schema that should be fufilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface Google_Gemma_3_12B_It_Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role?: string
content?: (string | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
}[] | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
})
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
/**
 * JSON schema that should be fufilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}

export type AiInput29 = (Qwen_Qwq_32B_Prompt | Qwen_Qwq_32B_Messages)

export interface Qwen_Qwq_32B_Prompt {
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * JSON schema that should be fulfilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}
export interface Qwen_Qwq_32B_Messages {
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role?: string
/**
 * The tool call id. Must be supplied for tool calls for Mistral-3. If you don't know what to put here you can fall back to 000000001
 */
tool_call_id?: string
content?: (string | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
}[] | {
/**
 * Type of the content provided
 */
type?: string
text?: string
image_url?: {
/**
 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
 */
url?: string
}
})
}[]
functions?: {
name: string
code: string
}[]
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
} | {
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
/**
 * Details of the function tool.
 */
function: {
/**
 * The name of the function.
 */
name: string
/**
 * A brief description of what the function does.
 */
description: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * The type of the parameters object (usually 'object').
 */
type: string
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * The data type of the parameter.
 */
type: string
/**
 * A description of the expected parameter.
 */
description: string
}
}
}
}
})[]
/**
 * JSON schema that should be fufilled for the response.
 */
guided_json?: {

}
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
}

export type AiInput30 = (_cf_meta_llama_3_1_70b_instruct_AiInput30_Prompt | _cf_meta_llama_3_1_70b_instruct_AiInput30_Messages)

export interface _cf_meta_llama_3_1_70b_instruct_AiInput30_Prompt {
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
image?: (number[] | string)
/**
 * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
 */
lora?: string
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
/**
 * The input text prompt for the model to generate a response.
 */
prompt: string
/**
 * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
 */
raw?: boolean
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
 */
stream?: boolean
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
}
export interface _cf_meta_llama_3_1_70b_instruct_AiInput30_Messages {
/**
 * Decreases the likelihood of the model repeating the same lines verbatim.
 */
frequency_penalty?: number
functions?: {
code: string
name: string
}[]
image?: (number[] | string)
/**
 * The maximum number of tokens to generate in the response.
 */
max_tokens?: number
/**
 * An array of message objects representing the conversation history.
 */
messages: {
/**
 * The content of the message as a string.
 */
content: string
/**
 * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
 */
role: string
}[]
/**
 * Increases the likelihood of the model introducing new topics.
 */
presence_penalty?: number
/**
 * Penalty for repeated tokens; higher values discourage repetition.
 */
repetition_penalty?: number
/**
 * Random seed for reproducibility of the generation.
 */
seed?: number
/**
 * If true, the response will be streamed back incrementally.
 */
stream?: boolean
/**
 * Controls the randomness of the output; higher values produce more random results.
 */
temperature?: number
/**
 * A list of tools available for the assistant to use.
 */
tools?: ({
/**
 * A brief description of what the tool does.
 */
description: string
/**
 * The name of the tool. More descriptive the better.
 */
name: string
/**
 * Schema defining the parameters accepted by the tool.
 */
parameters: {
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * A description of the expected parameter.
 */
description: string
/**
 * The data type of the parameter.
 */
type: string
}
}
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * The type of the parameters object (usually 'object').
 */
type: string
}
} | {
/**
 * Details of the function tool.
 */
function: {
/**
 * A brief description of what the function does.
 */
description: string
/**
 * The name of the function.
 */
name: string
/**
 * Schema defining the parameters accepted by the function.
 */
parameters: {
/**
 * Definitions of each parameter.
 */
properties: {
[k: string]: {
/**
 * A description of the expected parameter.
 */
description: string
/**
 * The data type of the parameter.
 */
type: string
}
}
/**
 * List of required parameter names.
 */
required?: string[]
/**
 * The type of the parameters object (usually 'object').
 */
type: string
}
}
/**
 * Specifies the type of tool (e.g., 'function').
 */
type: string
})[]
/**
 * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
 */
top_k?: number
/**
 * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
 */
top_p?: number
}

export interface AiInput31 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiOutput25 {
choices: {
message: {
role: "assistant"
content: string
}
finish_reason?: string
}[]
usage?: {
prompt_tokens?: number
completion_tokens?: number
total_tokens?: number
}
}

export interface AiInput32 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput33 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput34 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput35 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput36 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput37 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput38 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput39 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput40 {
/**
 * Audio file to transcribe
 */
file: string
/**
 * Model to use for transcription
 */
model?: string
/**
 * Language of the audio
 */
language?: string
/**
 * Optional prompt to guide transcription
 */
prompt?: string
response_format?: ("json" | "text" | "srt" | "verbose_json" | "vtt")
temperature?: number
}

export interface AiOutput26 {
/**
 * Transcribed text
 */
text: string
}

export interface AiInput41 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput42 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput43 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput44 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput45 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput46 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

export interface AiInput47 {
/**
 * Text to embed
 */
input: (string | string[])
/**
 * Model to use for embeddings
 */
model?: string
encoding_format?: ("float" | "base64")
/**
 * Number of dimensions in the embedding
 */
dimensions?: number
}

export interface AiOutput27 {
data: {
embedding: number[]
index: number
}[]
model?: string
usage?: {
prompt_tokens?: number
total_tokens?: number
}
}

export interface AiInput48 {
messages: {
role: ("system" | "user" | "assistant")
content: string
}[]
model?: string
temperature?: number
max_tokens?: number
stream?: boolean
response_format?: {
type?: ("json_object" | "text")
}
}

/** Generated input types for all AI models */
type _cf_pipecat_ai_smart_turn_v2_Input = AiInput;
type _cf_qwen_qwen1_5_0_5b_chat_Input = AiInput2;
type _cf_baai_bge_m3_Input = AiInput3;
type _cf_huggingface_distilbert_sst_2_int8_Input = AiInput4;
type _cf_google_gemma_2b_it_lora_Input = AiInput2;
type _hf_nexusflow_starling_lm_7b_beta_Input = AiInput2;
type _cf_meta_llama_3_8b_instruct_Input = AiInput2;
type _cf_meta_llama_3_2_3b_instruct_Input = AiInput2;
type _hf_thebloke_llamaguard_7b_awq_Input = AiInput2;
type _hf_thebloke_neural_chat_7b_v3_1_awq_Input = AiInput2;
type _cf_meta_llama_guard_3_8b_Input = AiInput5;
type _cf_meta_llama_2_7b_chat_fp16_Input = AiInput2;
type _cf_mistral_mistral_7b_instruct_v0_1_Input = AiInput2;
type _cf_myshell_ai_melotts_Input = AiInput6;
type _cf_mistral_mistral_7b_instruct_v0_2_lora_Input = AiInput2;
type _cf_openai_whisper_Input = AiInput7;
type _cf_tinyllama_tinyllama_1_1b_chat_v1_0_Input = AiInput2;
type _hf_mistral_mistral_7b_instruct_v0_2_Input = AiInput2;
type _cf_fblgit_una_cybertron_7b_v2_bf16_Input = AiInput2;
type _cf_llava_hf_llava_1_5_7b_hf_Input = AiInput8;
type _cf_deepseek_ai_deepseek_r1_distill_qwen_32b_Input = AiInput2;
type _cf_runwayml_stable_diffusion_v1_5_inpainting_Input = AiInput9;
type _cf_deepgram_nova_3_Input = AiInput10;
type _cf_black_forest_labs_flux_1_schnell_Input = AiInput11;
type _cf_thebloke_discolm_german_7b_v1_awq_Input = AiInput2;
type _cf_meta_llama_2_7b_chat_int8_Input = AiInput2;
type _cf_meta_llama_3_1_8b_instruct_fp8_Input = AiInput2;
type _hf_thebloke_mistral_7b_instruct_v0_1_awq_Input = AiInput2;
type _cf_qwen_qwen1_5_7b_chat_awq_Input = AiInput2;
type _cf_meta_llama_3_2_1b_instruct_Input = AiInput2;
type _hf_thebloke_llama_2_13b_chat_awq_Input = AiInput2;
type _cf_microsoft_resnet_50_Input = AiInput12;
type _cf_bytedance_stable_diffusion_xl_lightning_Input = AiInput9;
type _hf_thebloke_deepseek_coder_6_7b_base_awq_Input = AiInput2;
type _cf_meta_llama_llama_2_7b_chat_hf_lora_Input = AiInput2;
type _cf_meta_llama_3_3_70b_instruct_fp8_fast_Input = AiInput13;
type _cf_lykon_dreamshaper_8_lcm_Input = AiInput9;
type _cf_leonardo_phoenix_1_0_Input = AiInput14;
type _cf_stabilityai_stable_diffusion_xl_base_1_0_Input = AiInput9;
type _hf_thebloke_openhermes_2_5_mistral_7b_awq_Input = AiInput2;
type _cf_meta_m2m100_1_2b_Input = AiInput15;
type _hf_thebloke_deepseek_coder_6_7b_instruct_awq_Input = AiInput2;
type _cf_baai_bge_small_en_v1_5_Input = AiInput16;
type _cf_qwen_qwen2_5_coder_32b_instruct_Input = AiInput17;
type _cf_deepseek_ai_deepseek_math_7b_instruct_Input = AiInput2;
type _cf_tiiuae_falcon_7b_instruct_Input = AiInput2;
type _hf_nousresearch_hermes_2_pro_mistral_7b_Input = AiInput2;
type _cf_baai_bge_base_en_v1_5_Input = AiInput16;
type _cf_meta_llama_3_1_8b_instruct_awq_Input = AiInput2;
type _cf_unum_uform_gen2_qwen_500m_Input = AiInput18;
type _hf_thebloke_zephyr_7b_beta_awq_Input = AiInput2;
type _cf_google_gemma_7b_it_lora_Input = AiInput2;
type _cf_qwen_qwen1_5_1_8b_chat_Input = AiInput2;
type _cf_mistralai_mistral_small_3_1_24b_instruct_Input = AiInput19;
type _cf_meta_llama_3_8b_instruct_awq_Input = AiInput2;
type _cf_meta_llama_3_2_11b_vision_instruct_Input = AiInput20;
type _cf_openai_whisper_tiny_en_Input = AiInput7;
type _cf_openai_whisper_large_v3_turbo_Input = AiInput21;
type _cf_deepgram_aura_1_Input = AiInput22;
type _cf_defog_sqlcoder_7b_2_Input = AiInput2;
type _cf_microsoft_phi_2_Input = AiInput2;
type _hf_meta_llama_meta_llama_3_8b_instruct_Input = AiInput2;
type _cf_facebook_bart_large_cnn_Input = AiInput23;
type _cf_runwayml_stable_diffusion_v1_5_img2img_Input = AiInput9;
type _cf_google_embeddinggemma_300m_Input = AiInput24;
type _cf_baai_bge_reranker_base_Input = AiInput25;
type _hf_google_gemma_7b_it_Input = AiInput2;
type _cf_leonardo_lucid_origin_Input = AiInput26;
type _cf_qwen_qwen1_5_14b_chat_awq_Input = AiInput2;
type _cf_openchat_openchat_3_5_0106_Input = AiInput2;
type _cf_meta_llama_4_scout_17b_16e_instruct_Input = AiInput27;
type _cf_google_gemma_3_12b_it_Input = AiInput28;
type _cf_qwen_qwq_32b_Input = AiInput29;
type _cf_baai_bge_large_en_v1_5_Input = AiInput16;
type _cf_meta_llama_3_1_70b_instruct_Input = AiInput30;
type _cf_meta_llama_3_1_8b_instruct_fast_Input = AiInput30;
type llama_3_3_70b_Input = AiInput31;
type llama_4_maverick_17b_Input = AiInput32;
type llama_3_1_8b_instruct_Input = AiInput33;
type llama_3_1_70b_instruct_Input = AiInput34;
type deepseek_r1_Input = AiInput35;
type deepseek_v3_0324_Input = AiInput36;
type deepseek_r1_distill_llama_70b_Input = AiInput37;
type qwen_3_32b_Input = AiInput38;
type llama_3_3_swallow_70b_Input = AiInput39;
type whisper_large_v3_Input = AiInput40;
type llama_3_1_8b_instant_Input = AiInput41;
type gemma_9b_it_Input = AiInput42;
type kimi_k2_Input = AiInput43;
type gpt_oss_120b_Input = AiInput44;
type gpt_oss_20b_Input = AiInput45;
type gpt_oss_120b_test_Input = AiInput46;
type bge_embeddings_Input = AiInput47;
type piiranha_pii_Input = AiInput48;

/** Generated output types for all AI models */
type _cf_pipecat_ai_smart_turn_v2_Output = AiOutput;
type _cf_qwen_qwen1_5_0_5b_chat_Output = AiOutput2;
type _cf_baai_bge_m3_Output = AiOutput3;
type _cf_huggingface_distilbert_sst_2_int8_Output = AiOutput4;
type _cf_google_gemma_2b_it_lora_Output = AiOutput2;
type _hf_nexusflow_starling_lm_7b_beta_Output = AiOutput2;
type _cf_meta_llama_3_8b_instruct_Output = AiOutput2;
type _cf_meta_llama_3_2_3b_instruct_Output = AiOutput2;
type _hf_thebloke_llamaguard_7b_awq_Output = AiOutput2;
type _hf_thebloke_neural_chat_7b_v3_1_awq_Output = AiOutput2;
type _cf_meta_llama_guard_3_8b_Output = AiOutput5;
type _cf_meta_llama_2_7b_chat_fp16_Output = AiOutput2;
type _cf_mistral_mistral_7b_instruct_v0_1_Output = AiOutput2;
type _cf_myshell_ai_melotts_Output = AiOutput6;
type _cf_mistral_mistral_7b_instruct_v0_2_lora_Output = AiOutput2;
type _cf_openai_whisper_Output = AiOutput7;
type _cf_tinyllama_tinyllama_1_1b_chat_v1_0_Output = AiOutput2;
type _hf_mistral_mistral_7b_instruct_v0_2_Output = AiOutput2;
type _cf_fblgit_una_cybertron_7b_v2_bf16_Output = AiOutput2;
type _cf_llava_hf_llava_1_5_7b_hf_Output = AiOutput8;
type _cf_deepseek_ai_deepseek_r1_distill_qwen_32b_Output = AiOutput2;
type _cf_runwayml_stable_diffusion_v1_5_inpainting_Output = AiOutput9;
type _cf_deepgram_nova_3_Output = AiOutput10;
type _cf_black_forest_labs_flux_1_schnell_Output = AiOutput11;
type _cf_thebloke_discolm_german_7b_v1_awq_Output = AiOutput2;
type _cf_meta_llama_2_7b_chat_int8_Output = AiOutput2;
type _cf_meta_llama_3_1_8b_instruct_fp8_Output = AiOutput2;
type _hf_thebloke_mistral_7b_instruct_v0_1_awq_Output = AiOutput2;
type _cf_qwen_qwen1_5_7b_chat_awq_Output = AiOutput2;
type _cf_meta_llama_3_2_1b_instruct_Output = AiOutput2;
type _hf_thebloke_llama_2_13b_chat_awq_Output = AiOutput2;
type _cf_microsoft_resnet_50_Output = AiOutput12;
type _cf_bytedance_stable_diffusion_xl_lightning_Output = AiOutput9;
type _hf_thebloke_deepseek_coder_6_7b_base_awq_Output = AiOutput2;
type _cf_meta_llama_llama_2_7b_chat_hf_lora_Output = AiOutput2;
type _cf_meta_llama_3_3_70b_instruct_fp8_fast_Output = AiOutput13;
type _cf_lykon_dreamshaper_8_lcm_Output = AiOutput9;
type _cf_leonardo_phoenix_1_0_Output = AiOutput14;
type _cf_stabilityai_stable_diffusion_xl_base_1_0_Output = AiOutput9;
type _hf_thebloke_openhermes_2_5_mistral_7b_awq_Output = AiOutput2;
type _cf_meta_m2m100_1_2b_Output = AiOutput15;
type _hf_thebloke_deepseek_coder_6_7b_instruct_awq_Output = AiOutput2;
type _cf_baai_bge_small_en_v1_5_Output = AiOutput16;
type _cf_qwen_qwen2_5_coder_32b_instruct_Output = AiOutput2;
type _cf_deepseek_ai_deepseek_math_7b_instruct_Output = AiOutput2;
type _cf_tiiuae_falcon_7b_instruct_Output = AiOutput2;
type _hf_nousresearch_hermes_2_pro_mistral_7b_Output = AiOutput2;
type _cf_baai_bge_base_en_v1_5_Output = AiOutput16;
type _cf_meta_llama_3_1_8b_instruct_awq_Output = AiOutput2;
type _cf_unum_uform_gen2_qwen_500m_Output = AiOutput8;
type _hf_thebloke_zephyr_7b_beta_awq_Output = AiOutput2;
type _cf_google_gemma_7b_it_lora_Output = AiOutput2;
type _cf_qwen_qwen1_5_1_8b_chat_Output = AiOutput2;
type _cf_mistralai_mistral_small_3_1_24b_instruct_Output = AiOutput17;
type _cf_meta_llama_3_8b_instruct_awq_Output = AiOutput2;
type _cf_meta_llama_3_2_11b_vision_instruct_Output = AiOutput18;
type _cf_openai_whisper_tiny_en_Output = AiOutput7;
type _cf_openai_whisper_large_v3_turbo_Output = AiOutput19;
type _cf_deepgram_aura_1_Output = AiOutput20;
type _cf_defog_sqlcoder_7b_2_Output = AiOutput2;
type _cf_microsoft_phi_2_Output = AiOutput2;
type _hf_meta_llama_meta_llama_3_8b_instruct_Output = AiOutput2;
type _cf_facebook_bart_large_cnn_Output = AiOutput21;
type _cf_runwayml_stable_diffusion_v1_5_img2img_Output = AiOutput9;
type _cf_google_embeddinggemma_300m_Output = AiOutput22;
type _cf_baai_bge_reranker_base_Output = AiOutput23;
type _hf_google_gemma_7b_it_Output = AiOutput2;
type _cf_leonardo_lucid_origin_Output = AiOutput11;
type _cf_qwen_qwen1_5_14b_chat_awq_Output = AiOutput2;
type _cf_openchat_openchat_3_5_0106_Output = AiOutput2;
type _cf_meta_llama_4_scout_17b_16e_instruct_Output = AiOutput24;
type _cf_google_gemma_3_12b_it_Output = AiOutput17;
type _cf_qwen_qwq_32b_Output = AiOutput17;
type _cf_baai_bge_large_en_v1_5_Output = AiOutput16;
type _cf_meta_llama_3_1_70b_instruct_Output = AiOutput18;
type _cf_meta_llama_3_1_8b_instruct_fast_Output = AiOutput18;
type llama_3_3_70b_Output = AiOutput25;
type llama_4_maverick_17b_Output = AiOutput25;
type llama_3_1_8b_instruct_Output = AiOutput25;
type llama_3_1_70b_instruct_Output = AiOutput25;
type deepseek_r1_Output = AiOutput25;
type deepseek_v3_0324_Output = AiOutput25;
type deepseek_r1_distill_llama_70b_Output = AiOutput25;
type qwen_3_32b_Output = AiOutput25;
type llama_3_3_swallow_70b_Output = AiOutput25;
type whisper_large_v3_Output = AiOutput26;
type llama_3_1_8b_instant_Output = AiOutput25;
type gemma_9b_it_Output = AiOutput25;
type kimi_k2_Output = AiOutput25;
type gpt_oss_120b_Output = AiOutput25;
type gpt_oss_20b_Output = AiOutput25;
type gpt_oss_120b_test_Output = AiOutput25;
type bge_embeddings_Output = AiOutput27;
type piiranha_pii_Output = AiOutput25;

/** Unified input type mapping using conditional types based on model catalog */
export type AiModelInputMap = {
  [K in AvailableModel]: 
    K extends 'pii-detection' ? { prompt: string } :
    K extends 'bge-m3' | 'bge-small-en-v1.5' | 'bge-base-en-v1.5' | 'embeddinggemma-300m' | 'bge-large-en-v1.5' | 'embeddings' ? { input: string | string[]; model?: string; encoding_format?: 'float' | 'base64'; dimensions?: number } :
    K extends 'melotts' | 'aura-1' ? { text: string; model?: string; voice?: string; speed?: number; response_format?: 'mp3' | 'opus' | 'aac' | 'flac' | 'wav' | 'pcm' } :
    K extends 'smart-turn-v2' | 'whisper' | 'nova-3' | 'whisper-tiny-en' | 'whisper-large-v3-turbo' | 'whisper-large-v3' ? AudioInput :
    K extends 'stable-diffusion-v1-5-inpainting' | 'flux-1-schnell' | 'stable-diffusion-xl-lightning' | 'dreamshaper-8-lcm' | 'phoenix-1.0' | 'stable-diffusion-xl-base-1.0' | 'stable-diffusion-v1-5-img2img' | 'lucid-origin' ? ImageGenerationInput :
    K extends 'distilbert-sst-2-int8' | 'bge-reranker-base' ? TextClassificationInput :
    K extends 'resnet-50' ? ImageClassificationInput :
    K extends 'm2m100-1.2b' ? TranslationInput :
    K extends 'bart-large-cnn' ? SummarizationInput :
    K extends 'llava-1.5-7b-hf' | 'uform-gen2-qwen-500m' | 'llama-3.2-11b-vision-instruct' | 'llama-4-maverick-17b' ? VisionInput :
    K extends 'llama-3.3-70b' | 'llama-3.1-8b-external' | 'llama-3.1-70b-instruct' | 'deepseek-r1' | 'deepseek-v3-0324' | 'deepseek-r1-distill-llama-70b' | 'qwen-3-32b' | 'llama-3.3-swallow-70b' | 'llama-3.1-8b-instant' | 'gemma-9b-it' | 'kimi-k2' | 'gpt-oss-120b' | 'gpt-oss-20b' | 'gpt-oss-120b-test' ? OpenAIChatInputExternal :
    OpenAIChatInput;
};

/** Unified output type mapping using conditional types based on model catalog */
export type AiModelOutputMap = {
  [K in AvailableModel]: 
    K extends 'pii-detection' ? { pii_detection: Array<{ entity_type: string; text: string; start: number; end: number; confidence: number }> } :
    K extends 'bge-m3' | 'bge-small-en-v1.5' | 'bge-base-en-v1.5' | 'embeddinggemma-300m' | 'bge-large-en-v1.5' | 'embeddings' ? OpenAIEmbeddingOutput :
    K extends 'melotts' | 'aura-1' ? { audio: ArrayBuffer | Uint8Array; response_format?: string } :
    K extends 'smart-turn-v2' | 'whisper' | 'nova-3' | 'whisper-tiny-en' | 'whisper-large-v3-turbo' | 'whisper-large-v3' ? OpenAIAudioOutput :
    K extends 'stable-diffusion-v1-5-inpainting' | 'flux-1-schnell' | 'stable-diffusion-xl-lightning' | 'dreamshaper-8-lcm' | 'phoenix-1.0' | 'stable-diffusion-xl-base-1.0' | 'stable-diffusion-v1-5-img2img' | 'lucid-origin' ? ImageGenerationOutput :
    K extends 'distilbert-sst-2-int8' | 'bge-reranker-base' ? TextClassificationOutput :
    K extends 'resnet-50' ? ImageClassificationOutput :
    K extends 'm2m100-1.2b' ? TranslationOutput :
    K extends 'bart-large-cnn' ? SummarizationOutput :
    K extends 'llava-1.5-7b-hf' | 'uform-gen2-qwen-500m' | 'llama-3.2-11b-vision-instruct' | 'llama-4-maverick-17b' ? VisionOutput :
    OpenAIChatOutput;
};

/** Union of all available AI models */
export type AiModel = AvailableModel;

/** Generic async response type for queued AI requests */
export interface AiAsyncResponse {
  /** The async request id that can be used to obtain the results */
  request_id: string;
}

/** Main interface for AI operations
 * @remarks
 * Provides a unified interface for running various AI models across different tasks.
 * Each model type has its own specific input and output types.
 */
export interface Ai {
  run<T extends AiModel, I extends AiModelInputMap[T] = AiModelInputMap[T], O extends ExtendedAiOptions = ExtendedAiOptions>(
    model: T,
    inputs: I,
    options?: O,
  ): Promise<
    I extends { stream: true } ? ReadableStream :
    O extends { returnRawResponse: true } ? Response :
    O extends { queueRequest: true } ? AiAsyncResponse :
    AiModelOutputMap[T]
  >;
}
