/** Complete catalog of all available AI models */
export declare const MODEL_CATALOG: readonly [{
    readonly name: "smart-turn-v2";
    readonly capability: "audio";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/pipecat-ai/smart-turn-v2";
    readonly description: "An open source, community-driven, native audio turn detection model in 2nd version";
}, {
    readonly name: "gpt-oss-120b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/openai/gpt-oss-120b";
    readonly description: "OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases – gpt-oss-120b is for production, general purpose, high reasoning use-cases.";
}, {
    readonly name: "qwen-1.5-0.5b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/qwen/qwen1.5-0.5b-chat";
    readonly description: "Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud.";
}, {
    readonly name: "bge-m3";
    readonly capability: "embeddings";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/baai/bge-m3";
    readonly description: "Multi-Functionality, Multi-Linguality, and Multi-Granularity embeddings model.";
}, {
    readonly name: "distilbert-sst-2-int8";
    readonly capability: "text-classification";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/huggingface/distilbert-sst-2-int8";
    readonly description: "Distilled BERT model that was finetuned on SST-2 for sentiment classification";
}, {
    readonly name: "gemma-2b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/google/gemma-2b-it-lora";
    readonly description: "This is a Gemma-2B base model that Cloudflare dedicates for inference with LoRA adapters. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.";
}, {
    readonly name: "starling-lm-7b-beta";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/nexusflow/starling-lm-7b-beta";
    readonly description: "We introduce Starling-LM-7B-beta, an open large language model (LLM) trained by Reinforcement Learning from AI Feedback (RLAIF). Starling-LM-7B-beta is trained from Openchat-3.5-0106 with our new reward model Nexusflow/Starling-RM-34B and policy optimization method Fine-Tuning Language Models from Human Preferences (PPO).";
}, {
    readonly name: "llama-3-8b-instruct";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3-8b-instruct";
    readonly description: "Generation over generation, Meta Llama 3 demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning.";
}, {
    readonly name: "llama-3.2-3b-instruct";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.2-3b-instruct";
    readonly description: "The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.";
}, {
    readonly name: "llamaguard-7b-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/llamaguard-7b-awq";
    readonly description: "Llama Guard is a model for classifying the safety of LLM prompts and responses, using a taxonomy of safety risks. ";
}, {
    readonly name: "neural-chat-7b-v3-1-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/neural-chat-7b-v3-1-awq";
    readonly description: "This model is a fine-tuned 7B parameter LLM on the Intel Gaudi 2 processor from the mistralai/Mistral-7B-v0.1 on the open source dataset Open-Orca/SlimOrca.";
}, {
    readonly name: "llama-guard-3-8b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-guard-3-8b";
    readonly description: "Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.";
}, {
    readonly name: "llama-2-7b-chat-fp16";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-2-7b-chat-fp16";
    readonly description: "Full precision (fp16) generative text model with 7 billion parameters from Meta";
}, {
    readonly name: "mistral-7b-instruct";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/mistral/mistral-7b-instruct-v0.1";
    readonly description: "Instruct fine-tuned version of the Mistral-7b generative text model with 7 billion parameters";
}, {
    readonly name: "melotts";
    readonly capability: "tts";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/myshell-ai/melotts";
    readonly description: "MeloTTS is a high-quality multi-lingual text-to-speech library by MyShell.ai.";
}, {
    readonly name: "mistral-7b-instruct-v0.2-lora";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/mistral/mistral-7b-instruct-v0.2-lora";
    readonly description: "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.";
}, {
    readonly name: "whisper";
    readonly capability: "audio";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/openai/whisper";
    readonly description: "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.";
}, {
    readonly name: "tinyllama-1.1b-chat-v1.0";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/tinyllama/tinyllama-1.1b-chat-v1.0";
    readonly description: "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. This is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T.";
}, {
    readonly name: "mistral-7b-instruct-v0.2";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/mistral/mistral-7b-instruct-v0.2";
    readonly description: "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2. Mistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1: 32k context window (vs 8k context in v0.1), rope-theta = 1e6, and no Sliding-Window Attention.";
}, {
    readonly name: "una-cybertron-7b-v2-bf16";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/fblgit/una-cybertron-7b-v2-bf16";
    readonly description: "Cybertron 7B v2 is a 7B MistralAI based model, best on it's series. It was trained with SFT, DPO and UNA (Unified Neural Alignment) on multiple datasets.";
}, {
    readonly name: "llava-1.5-7b";
    readonly capability: "vision";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/llava-hf/llava-1.5-7b-hf";
    readonly description: "LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture.";
}, {
    readonly name: "deepseek-r1-distill-qwen-32b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b";
    readonly description: "DeepSeek-R1-Distill-Qwen-32B is a model distilled from DeepSeek-R1 based on Qwen2.5. It outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.";
}, {
    readonly name: "stable-diffusion-v1-5-inpainting";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/runwayml/stable-diffusion-v1-5-inpainting";
    readonly description: "Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.";
}, {
    readonly name: "nova-3";
    readonly capability: "audio";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/deepgram/nova-3";
    readonly description: "Transcribe audio using Deepgram’s speech-to-text model";
}, {
    readonly name: "flux-1-schnell";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/black-forest-labs/flux-1-schnell";
    readonly description: "FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. ";
}, {
    readonly name: "discolm-german-7b-v1-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/thebloke/discolm-german-7b-v1-awq";
    readonly description: "DiscoLM German 7b is a Mistral-based large language model with a focus on German-language applications. AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization.";
}, {
    readonly name: "llama-2-7b-chat-int8";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-2-7b-chat-int8";
    readonly description: "Quantized (int8) generative text model with 7 billion parameters from Meta";
}, {
    readonly name: "llama-3.1-8b-instruct-fp8";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.1-8b-instruct-fp8";
    readonly description: "Llama 3.1 8B quantized to FP8 precision";
}, {
    readonly name: "mistral-7b-instruct-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/mistral-7b-instruct-v0.1-awq";
    readonly description: "Mistral 7B Instruct v0.1 AWQ is an efficient, accurate and blazing-fast low-bit weight quantized Mistral variant.";
}, {
    readonly name: "qwen-1.5-7b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/qwen/qwen1.5-7b-chat-awq";
    readonly description: "Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud. AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization.";
}, {
    readonly name: "llama-3.2-1b-instruct";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.2-1b-instruct";
    readonly description: "The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks.";
}, {
    readonly name: "llama-2-13b-chat-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/llama-2-13b-chat-awq";
    readonly description: "Llama 2 13B Chat AWQ is an efficient, accurate and blazing-fast low-bit weight quantized Llama 2 variant.";
}, {
    readonly name: "resnet-50";
    readonly capability: "image-classification";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/microsoft/resnet-50";
    readonly description: "50 layers deep image classification CNN trained on more than 1M images from ImageNet";
}, {
    readonly name: "stable-diffusion-xl-lightning";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/bytedance/stable-diffusion-xl-lightning";
    readonly description: "SDXL-Lightning is a lightning-fast text-to-image generation model. It can generate high-quality 1024px images in a few steps.";
}, {
    readonly name: "deepseek-coder-6.7b-base";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/deepseek-coder-6.7b-base-awq";
    readonly description: "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.";
}, {
    readonly name: "llama-2-7b-chat-hf-lora";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta-llama/llama-2-7b-chat-hf-lora";
    readonly description: "This is a Llama2 base model that Cloudflare dedicated for inference with LoRA adapters. Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. ";
}, {
    readonly name: "llama-3.3-70b-instruct-fp8";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.3-70b-instruct-fp8-fast";
    readonly description: "Llama 3.3 70B quantized to fp8 precision, optimized to be faster.";
}, {
    readonly name: "dreamshaper-8-lcm";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/lykon/dreamshaper-8-lcm";
    readonly description: "Stable Diffusion model that has been fine-tuned to be better at photorealism without sacrificing range.";
}, {
    readonly name: "phoenix-1.0";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/leonardo/phoenix-1.0";
    readonly description: "Phoenix 1.0 is a model by Leonardo.Ai that generates images with exceptional prompt adherence and coherent text.";
}, {
    readonly name: "stable-diffusion-xl-base-1.0";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/stabilityai/stable-diffusion-xl-base-1.0";
    readonly description: "Diffusion-based text-to-image generative model by Stability AI. Generates and modify images based on text prompts.";
}, {
    readonly name: "openhermes-2.5-mistral-7b-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/openhermes-2.5-mistral-7b-awq";
    readonly description: "OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.";
}, {
    readonly name: "m2m100-1.2b";
    readonly capability: "translation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/m2m100-1.2b";
    readonly description: "Multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation";
}, {
    readonly name: "deepseek-coder-6.7b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/deepseek-coder-6.7b-instruct-awq";
    readonly description: "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.";
}, {
    readonly name: "bge-small-en";
    readonly capability: "embeddings";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/baai/bge-small-en-v1.5";
    readonly description: "BAAI general embedding (Small) model that transforms any given text into a 384-dimensional vector";
}, {
    readonly name: "qwen-coder-32b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/qwen/qwen2.5-coder-32b-instruct";
    readonly description: "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:";
}, {
    readonly name: "deepseek-math-7b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/deepseek-ai/deepseek-math-7b-instruct";
    readonly description: "DeepSeekMath-Instruct 7B is a mathematically instructed tuning model derived from DeepSeekMath-Base 7B. DeepSeekMath is initialized with DeepSeek-Coder-v1.5 7B and continues pre-training on math-related tokens sourced from Common Crawl, together with natural language and code data for 500B tokens.";
}, {
    readonly name: "falcon-7b-instruct";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/tiiuae/falcon-7b-instruct";
    readonly description: "Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets.";
}, {
    readonly name: "hermes-2-pro-mistral-7b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/nousresearch/hermes-2-pro-mistral-7b";
    readonly description: "Hermes 2 Pro on Mistral 7B is the new flagship 7B Hermes! Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.";
}, {
    readonly name: "bge-base-en";
    readonly capability: "embeddings";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/baai/bge-base-en-v1.5";
    readonly description: "BAAI general embedding (Base) model that transforms any given text into a 768-dimensional vector";
}, {
    readonly name: "llama-3.1-8b-instruct-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.1-8b-instruct-awq";
    readonly description: "Quantized (int4) generative text model with 8 billion parameters from Meta. ";
}, {
    readonly name: "uform-gen2-qwen-500m";
    readonly capability: "vision";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/unum/uform-gen2-qwen-500m";
    readonly description: "UForm-Gen is a small generative vision-language model primarily designed for Image Captioning and Visual Question Answering. The model was pre-trained on the internal image captioning dataset and fine-tuned on public instructions datasets: SVIT, LVIS, VQAs datasets.";
}, {
    readonly name: "zephyr-7b-beta-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/thebloke/zephyr-7b-beta-awq";
    readonly description: "Zephyr 7B Beta AWQ is an efficient, accurate and blazing-fast low-bit weight quantized Zephyr model variant.";
}, {
    readonly name: "gemma-7b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/google/gemma-7b-it-lora";
    readonly description: "  This is a Gemma-7B base model that Cloudflare dedicates for inference with LoRA adapters. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.";
}, {
    readonly name: "qwen-1.5-1.8b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/qwen/qwen1.5-1.8b-chat";
    readonly description: "Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud.";
}, {
    readonly name: "mistral-small-3.1";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/mistralai/mistral-small-3.1-24b-instruct";
    readonly description: "Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.";
}, {
    readonly name: "llama-3-8b-instruct-awq";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3-8b-instruct-awq";
    readonly description: "Quantized (int4) generative text model with 8 billion parameters from Meta.";
}, {
    readonly name: "llama-3.2-11b-vision";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.2-11b-vision-instruct";
    readonly description: " The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image.";
}, {
    readonly name: "whisper-tiny";
    readonly capability: "audio";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/openai/whisper-tiny-en";
    readonly description: "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. This is the English-only version of the Whisper Tiny model which was trained on the task of speech recognition.";
}, {
    readonly name: "whisper-large-v3-turbo";
    readonly capability: "audio";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/openai/whisper-large-v3-turbo";
    readonly description: "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. ";
}, {
    readonly name: "aura-1";
    readonly capability: "tts";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/deepgram/aura-1";
    readonly description: "Aura is a context-aware text-to-speech (TTS) model that applies natural pacing, expressiveness, and fillers based on the context of the provided text. The quality of your text input directly impacts the naturalness of the audio output.";
}, {
    readonly name: "sqlcoder-7b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/defog/sqlcoder-7b-2";
    readonly description: "This model is intended to be used by non-technical users to understand data inside their SQL databases. ";
}, {
    readonly name: "phi-2";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/microsoft/phi-2";
    readonly description: "Phi-2 is a Transformer-based model with a next-word prediction objective, trained on 1.4T tokens from multiple passes on a mixture of Synthetic and Web datasets for NLP and coding.";
}, {
    readonly name: "meta-llama-3-8b-instruct";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/meta-llama/meta-llama-3-8b-instruct";
    readonly description: "Generation over generation, Meta Llama 3 demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning.\t";
}, {
    readonly name: "bart-large-cnn";
    readonly capability: "summarization";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/facebook/bart-large-cnn";
    readonly description: "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. You can use this model for text summarization.";
}, {
    readonly name: "stable-diffusion-v1-5-img2img";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/runwayml/stable-diffusion-v1-5-img2img";
    readonly description: "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images. Img2img generate a new image from an input image with Stable Diffusion. ";
}, {
    readonly name: "gpt-oss-20b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/openai/gpt-oss-20b";
    readonly description: "OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases – gpt-oss-20b is for lower latency, and local or specialized use-cases.";
}, {
    readonly name: "embeddinggemma-300m";
    readonly capability: "embeddings";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/google/embeddinggemma-300m";
    readonly description: "EmbeddingGemma is a 300M parameter, state-of-the-art for its size, open embedding model from Google, built from Gemma 3 (with T5Gemma initialization) and the same research and technology used to create Gemini models. EmbeddingGemma produces vector representations of text, making it well-suited for search and retrieval tasks, including classification, clustering, and semantic similarity search. This model was trained with data in 100+ spoken languages.";
}, {
    readonly name: "bge-reranker-base";
    readonly capability: "text-classification";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/baai/bge-reranker-base";
    readonly description: "Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. You can get a relevance score by inputting query and passage to the reranker. And the score can be mapped to a float value in [0,1] by sigmoid function.  ";
}, {
    readonly name: "gemma-7b-it";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@hf/google/gemma-7b-it";
    readonly description: "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants.";
}, {
    readonly name: "lucid-origin";
    readonly capability: "image-generation";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/leonardo/lucid-origin";
    readonly description: "Lucid Origin from Leonardo.AI is their most adaptable and prompt-responsive model to date. Whether you're generating images with sharp graphic design, stunning full-HD renders, or highly specific creative direction, it adheres closely to your prompts, renders text with accuracy, and supports a wide array of visual styles and aesthetics – from stylized concept art to crisp product mockups. ";
}, {
    readonly name: "qwen-1.5-14b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/qwen/qwen1.5-14b-chat-awq";
    readonly description: "Qwen1.5 is the improved version of Qwen, the large language model series developed by Alibaba Cloud. AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization.";
}, {
    readonly name: "openchat-3.5";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/openchat/openchat-3.5-0106";
    readonly description: "OpenChat is an innovative library of open-source language models, fine-tuned with C-RLFT - a strategy inspired by offline reinforcement learning.";
}, {
    readonly name: "llama-4-scout-17b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-4-scout-17b-16e-instruct";
    readonly description: "Meta's Llama 4 Scout is a 17 billion parameter model with 16 experts that is natively multimodal. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.";
}, {
    readonly name: "gemma-3-12b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/google/gemma-3-12b-it";
    readonly description: "Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Gemma 3 models are multimodal, handling text and image input and generating text output, with a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions.";
}, {
    readonly name: "qwen-qwq-32b";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/qwen/qwq-32b";
    readonly description: "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.";
}, {
    readonly name: "bge-large-en";
    readonly capability: "embeddings";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/baai/bge-large-en-v1.5";
    readonly description: "BAAI general embedding (Large) model that transforms any given text into a 1024-dimensional vector";
}, {
    readonly name: "llama-3.1-70b-instruct";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.1-70b-instruct";
    readonly description: "Llama 3.1 70B Instruct";
}, {
    readonly name: "llama-3.1-8b-instruct-fast";
    readonly capability: "chat";
    readonly provider: "cloudflare";
    readonly providerModel: "@cf/meta/llama-3.1-8b-instruct-fast";
    readonly description: "Llama 3.1 8B Instruct Fast";
}, {
    readonly name: "llama-3.3-70b";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "llama-3.3-70b";
    readonly description: "External model routed through model-router: llama-3.3-70b";
}, {
    readonly name: "llama-4-maverick-17b";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "llama-4-maverick-17b";
    readonly description: "External model routed through model-router: llama-4-maverick-17b";
}, {
    readonly name: "llama-3.1-8b-external";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "llama-3.1-8b-instruct";
    readonly description: "External model routed through model-router: llama-3.1-8b-instruct";
}, {
    readonly name: "deepseek-r1";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "deepseek-r1";
    readonly description: "External model routed through model-router: deepseek-r1";
}, {
    readonly name: "deepseek-v3-0324";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "deepseek-v3-0324";
    readonly description: "External model routed through model-router: deepseek-v3-0324";
}, {
    readonly name: "deepseek-r1-distill-llama-70b";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "deepseek-r1-distill-llama-70b";
    readonly description: "External model routed through model-router: deepseek-r1-distill-llama-70b";
}, {
    readonly name: "qwen-3-32b";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "qwen-3-32b";
    readonly description: "External model routed through model-router: qwen-3-32b";
}, {
    readonly name: "llama-3.3-swallow-70b";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "llama-3.3-swallow-70b";
    readonly description: "External model routed through model-router: llama-3.3-swallow-70b";
}, {
    readonly name: "whisper-large-v3";
    readonly capability: "audio";
    readonly provider: "external";
    readonly providerModel: "whisper-large-v3";
    readonly description: "External model routed through model-router: whisper-large-v3";
}, {
    readonly name: "llama-3.1-8b-instant";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "llama-3.1-8b-instant";
    readonly description: "External model routed through model-router: llama-3.1-8b-instant";
}, {
    readonly name: "gemma-9b-it";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "gemma-9b-it";
    readonly description: "External model routed through model-router: gemma-9b-it";
}, {
    readonly name: "kimi-k2";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "kimi-k2";
    readonly description: "External model routed through model-router: kimi-k2";
}, {
    readonly name: "gpt-oss-120b-test";
    readonly capability: "chat";
    readonly provider: "external";
    readonly providerModel: "gpt-oss-120b-test";
    readonly description: "External model routed through model-router: gpt-oss-120b-test";
}, {
    readonly name: "embeddings";
    readonly capability: "embeddings";
    readonly provider: "external";
    readonly providerModel: "bge-embeddings";
    readonly description: "External model routed through model-router: bge-embeddings";
}, {
    readonly name: "pii-detection";
    readonly capability: "pii-detection";
    readonly provider: "external";
    readonly providerModel: "piiranha-pii";
    readonly description: "External model routed through model-router: piiranha-pii";
}];
/** Available model names */
export type AvailableModel = typeof MODEL_CATALOG[number]['name'];
/** Model capabilities */
export type ModelCapability = 'chat' | 'embeddings' | 'audio' | 'tts' | 'pii-detection' | 'image-generation' | 'text-classification' | 'image-classification' | 'translation' | 'summarization' | 'vision';
/** Model providers */
export type ModelProvider = 'cloudflare' | 'external';
/** Model catalog entry */
export interface ModelCatalogEntry {
    name: string;
    capability: ModelCapability;
    provider: ModelProvider;
    providerModel: string;
    description: string;
}
/** Get all models by capability */
export declare function getModelsByCapability(capability: ModelCapability): ModelCatalogEntry[];
/** Get model information by name */
export declare function getModelInfo(modelName: AvailableModel): ModelCatalogEntry | undefined;
/** OpenAI-compatible chat input interface */
export interface OpenAIChatInput {
    messages: Array<{
        role: 'system' | 'user' | 'assistant';
        content: string;
    }>;
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type: 'json_object' | 'text';
    };
}
/** OpenAI-compatible chat input interface for external models (requires model field) */
export interface OpenAIChatInputExternal extends Omit<OpenAIChatInput, 'model'> {
    model: string;
}
/** OpenAI-compatible chat output interface */
export interface OpenAIChatOutput {
    choices: Array<{
        message: {
            role: 'assistant';
            content: string;
        };
        finish_reason?: string;
    }>;
    usage?: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
}
/** OpenAI-compatible embedding output interface */
export interface OpenAIEmbeddingOutput {
    data: Array<{
        embedding: number[];
        index: number;
    }>;
    model?: string;
    usage?: {
        prompt_tokens: number;
        total_tokens: number;
    };
}
/** OpenAI-compatible audio output interface */
export interface OpenAIAudioOutput {
    text: string;
}
/** Image generation input interface */
export interface ImageGenerationInput {
    prompt: string;
    model?: string;
    n?: number;
    size?: string;
    quality?: string;
    style?: string;
    response_format?: 'url' | 'b64_json';
}
/** Image generation output interface */
export interface ImageGenerationOutput {
    data: Array<{
        url?: string;
        b64_json?: string;
    }>;
}
/** Text classification input interface */
export interface TextClassificationInput {
    text: string;
    model?: string;
}
/** Text classification output interface */
export interface TextClassificationOutput {
    label: string;
    score: number;
}
/** Image classification input interface */
export interface ImageClassificationInput {
    image: File | Blob | string;
    model?: string;
}
/** Image classification output interface */
export interface ImageClassificationOutput {
    label: string;
    score: number;
}
/** Translation input interface */
export interface TranslationInput {
    text: string;
    source_lang?: string;
    target_lang: string;
    model?: string;
}
/** Translation output interface */
export interface TranslationOutput {
    translation: string;
    source_lang?: string;
    target_lang: string;
}
/** Summarization input interface */
export interface SummarizationInput {
    text: string;
    max_length?: number;
    min_length?: number;
    model?: string;
}
/** Summarization output interface */
export interface SummarizationOutput {
    summary: string;
}
/** Vision input interface */
export interface VisionInput {
    messages: Array<{
        role: 'system' | 'user' | 'assistant';
        content: Array<{
            type: 'text' | 'image_url';
            text?: string;
            image_url?: {
                url: string;
                detail?: 'low' | 'high' | 'auto';
            };
        }>;
    }>;
    model: string;
    max_tokens?: number;
    temperature?: number;
    stream?: boolean;
}
/** Vision output interface */
export interface VisionOutput {
    choices: Array<{
        message: {
            role: 'assistant';
            content: string;
        };
        finish_reason?: string;
    }>;
}
/** Configuration options for AI gateway requests */
export type GatewayOptions = {
    /** Unique identifier for the request */
    id: string;
    /** Cache key for the request */
    cacheKey?: string;
    /** Time-to-live in seconds for cache entries */
    cacheTtl?: number;
    /** Whether to bypass cache for this request */
    skipCache?: boolean;
    /** Additional metadata to attach to the request */
    metadata?: Record<string, number | string | boolean | null | bigint>;
    /** Whether to collect logs for this request */
    collectLog?: boolean;
    /** Event ID for tracking */
    eventId?: string;
    /** Request timeout in milliseconds */
    requestTimeoutMs?: number;
};
/** Retry configuration for gateway requests */
export type GatewayRetries = {
    /** Maximum number of retries */
    maxRetries?: number;
    /** Backoff strategy */
    backoffStrategy?: 'exponential' | 'linear' | 'fixed';
    /** Initial delay in milliseconds */
    initialDelay?: number;
    /** Maximum delay in milliseconds */
    maxDelay?: number;
};
/** General options for AI operations */
export type AiOptions = {
    /** Process the request asynchronously as a batch */
    queueRequest?: boolean;
    /** Return the raw Response object instead of parsed data */
    returnRawResponse?: boolean;
    /** Gateway-specific configuration */
    gateway?: GatewayOptions;
    /** URL prefix for API endpoints */
    prefix?: string;
    /** Additional HTTP headers to include */
    extraHeaders?: Record<string, string>;
};
/** Extended options for AI operations including smart bucket authentication */
export interface ExtendedAiOptions extends AiOptions {
    includeTimingData?: boolean;
    stream?: boolean;
    smartBucketAuth?: {
        bucketId: string;
        secret: string;
    };
}
/** Reranker models (bge-reranker-base, etc.) */
export interface RerankerInput {
    query: string;
    documents: string[];
    top_k?: number;
}
/** Summarization models (bart-large-cnn, etc.) */
export interface SummarizationInput {
    text: string;
    max_length?: number;
    min_length?: number;
}
/** Translation models (m2m100-1.2b, etc.) */
export interface TranslationInput {
    text: string;
    source_language?: string;
    target_language: string;
}
/** Text Classification models (distilbert-sst-2-int8, etc.) */
export interface TextClassificationInput {
    text: string;
    labels?: string[];
}
/** Image Generation models (flux-1-schnell, stable-diffusion, etc.) */
export interface ImageGenerationInput {
    prompt: string;
    negative_prompt?: string;
    width?: number;
    height?: number;
    steps?: number;
    guidance_scale?: number;
}
/** Image Classification models (resnet-50, etc.) */
export interface ImageClassificationInput {
    image: string | File | Blob;
    prompt?: string;
}
/** Vision models (llava-1.5-7b, etc.) */
export interface VisionInput {
    messages: Array<{
        role: 'system' | 'user' | 'assistant';
        content: Array<{
            type: 'text' | 'image_url';
            text?: string;
            image_url?: {
                url: string;
                detail?: 'low' | 'high' | 'auto';
            };
        }>;
    }>;
    model: string;
    max_tokens?: number;
    temperature?: number;
    stream?: boolean;
}
/** TTS models (aura-1, melotts, etc.) */
export interface TTSInput {
    text: string;
    voice?: string;
    speed?: number;
    response_format?: 'mp3' | 'wav' | 'ogg';
}
/** Audio models (whisper variants, etc.) - Supports both array format for RPC serialization and ReadableStream for large files */
export interface AudioInput {
    audio: number[] | ReadableStream<Uint8Array>;
    contentType: string;
    language?: string;
    prompt?: string;
    response_format?: 'json' | 'text' | 'srt' | 'vtt';
}
/** Reranker output interface */
export interface RerankerOutput {
    ranked_documents: Array<{
        index: number;
        document: string;
        relevance_score: number;
    }>;
}
/** Summarization output interface */
export interface SummarizationOutput {
    summary: string;
}
/** Translation output interface */
export interface TranslationOutput {
    translation: string;
    source_lang?: string;
    target_lang: string;
}
/** Text classification output interface */
export interface TextClassificationOutput {
    label: string;
    score: number;
}
/** Image generation output interface */
export interface ImageGenerationOutput {
    data: Array<{
        url?: string;
        b64_json?: string;
    }>;
}
/** Image classification output interface */
export interface ImageClassificationOutput {
    label: string;
    score: number;
}
/** Vision output interface */
export interface VisionOutput {
    choices: Array<{
        message: {
            role: 'assistant';
            content: string;
        };
        finish_reason?: string;
    }>;
}
/** TTS output interface */
export interface TTSOutput {
    audio: ArrayBuffer | Uint8Array;
    response_format?: string;
}
/** Audio output interface */
export interface AudioOutput {
    text: string;
}
export type AiInput = ({
    /**
     * readable stream with audio data and content-type specified for that data
     */
    audio: {
        body: {};
        contentType: string;
    };
    /**
     * type of data PCM data that's sent to the inference server as raw array
     */
    dtype?: ("uint8" | "float32" | "float64");
} | {
    /**
     * base64 encoded audio data
     */
    audio: string;
    /**
     * type of data PCM data that's sent to the inference server as raw array
     */
    dtype?: ("uint8" | "float32" | "float64");
});
export interface AiOutput {
    /**
     * if true, end-of-turn was detected
     */
    is_complete?: boolean;
    /**
     * probability of the end-of-turn detection
     */
    probability?: number;
}
export type AiInput2 = (Prompt | Messages);
export interface Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
     */
    lora?: string;
    response_format?: JSONMode;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface JSONMode {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role: string;
        /**
         * The content of the message as a string.
         */
        content: string;
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    response_format?: JSONMode1;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface JSONMode1 {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface AiOutput2 {
    /**
     * The generated text response from the model
     */
    response: string;
    /**
     * Usage statistics for the inference request
     */
    usage?: {
        /**
         * Total number of tokens in input
         */
        prompt_tokens?: number;
        /**
         * Total number of tokens in output
         */
        completion_tokens?: number;
        /**
         * Total number of input and output tokens
         */
        total_tokens?: number;
    };
    /**
     * An array of tool calls requests made during the response generation
     */
    tool_calls?: {
        /**
         * The arguments passed to be passed to the tool call request
         */
        arguments?: {};
        /**
         * The name of the tool to be called
         */
        name?: string;
    }[];
}
export type AiInput3 = (BGEM3InputQueryAndContexts | BGEM3InputEmbedding | {
    /**
     * Batch of the embeddings requests to run using async-queue
     */
    requests: (BGEM3InputQueryAndContexts1 | BGEM3InputEmbedding1)[];
});
export interface BGEM3InputQueryAndContexts {
    /**
     * A query you wish to perform against the provided contexts. If no query is provided the model with respond with embeddings for contexts
     */
    query?: string;
    /**
     * List of provided contexts. Note that the index in this array is important, as the response will refer to it.
     */
    contexts: {
        /**
         * One of the provided context content
         */
        text?: string;
    }[];
    /**
     * When provided with too long context should the model error out or truncate the context to fit?
     */
    truncate_inputs?: boolean;
}
export interface BGEM3InputEmbedding {
    text: (string | string[]);
    /**
     * When provided with too long context should the model error out or truncate the context to fit?
     */
    truncate_inputs?: boolean;
}
export interface BGEM3InputQueryAndContexts1 {
    /**
     * A query you wish to perform against the provided contexts. If no query is provided the model with respond with embeddings for contexts
     */
    query?: string;
    /**
     * List of provided contexts. Note that the index in this array is important, as the response will refer to it.
     */
    contexts: {
        /**
         * One of the provided context content
         */
        text?: string;
    }[];
    /**
     * When provided with too long context should the model error out or truncate the context to fit?
     */
    truncate_inputs?: boolean;
}
export interface BGEM3InputEmbedding1 {
    text: (string | string[]);
    /**
     * When provided with too long context should the model error out or truncate the context to fit?
     */
    truncate_inputs?: boolean;
}
export type AiOutput3 = (BGEM3OuputQuery | BGEM3OutputEmbeddingForContexts | BGEM3OuputEmbedding);
export interface BGEM3OuputQuery {
    response?: {
        /**
         * Index of the context in the request
         */
        id?: number;
        /**
         * Score of the context under the index.
         */
        score?: number;
    }[];
}
export interface BGEM3OutputEmbeddingForContexts {
    response?: number[][];
    shape?: number[];
    /**
     * The pooling method used in the embedding process.
     */
    pooling?: ("mean" | "cls");
}
export interface BGEM3OuputEmbedding {
    shape?: number[];
    /**
     * Embeddings of the requested text values
     */
    data?: number[][];
    /**
     * The pooling method used in the embedding process.
     */
    pooling?: ("mean" | "cls");
}
export interface AiInput4 {
    /**
     * The text that you want to classify
     */
    text: string;
}
/**
 * An array of classification results for the input text
 */
export type AiOutput4 = {
    /**
     * Confidence score indicating the likelihood that the text belongs to the specified label
     */
    score?: number;
    /**
     * The classification label assigned to the text (e.g., 'POSITIVE' or 'NEGATIVE')
     */
    label?: string;
}[];
export interface AiInput5 {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender must alternate between 'user' and 'assistant'.
         */
        role: ("user" | "assistant");
        /**
         * The content of the message as a string.
         */
        content: string;
    }[];
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Dictate the output format of the generated response.
     */
    response_format?: {
        /**
         * Set to json_object to process and output generated text as JSON.
         */
        type?: string;
    };
}
export interface AiOutput5 {
    response?: (string | {
        /**
         * Whether the conversation is safe or not.
         */
        safe?: boolean;
        /**
         * A list of what hazard categories predicted for the conversation, if the conversation is deemed unsafe.
         */
        categories?: string[];
    });
    /**
     * Usage statistics for the inference request
     */
    usage?: {
        /**
         * Total number of tokens in input
         */
        prompt_tokens?: number;
        /**
         * Total number of tokens in output
         */
        completion_tokens?: number;
        /**
         * Total number of input and output tokens
         */
        total_tokens?: number;
    };
}
export interface AiInput6 {
    /**
     * A text description of the audio you want to generate
     */
    prompt: string;
    /**
     * The speech language (e.g., 'en' for English, 'fr' for French). Defaults to 'en' if not specified
     */
    lang?: string;
}
export interface AiOutput6 {
    /**
     * The generated audio in MP3 format, base64-encoded
     */
    audio?: string;
}
export type AiInput7 = (string | {
    /**
     * An array of integers that represent the audio data constrained to 8-bit unsigned integer values
     */
    audio: number[];
});
export interface AiOutput7 {
    /**
     * The transcription
     */
    text: string;
    word_count?: number;
    words?: {
        word?: string;
        /**
         * The second this word begins in the recording
         */
        start?: number;
        /**
         * The ending second when the word completes
         */
        end?: number;
    }[];
    vtt?: string;
}
export type AiInput8 = (string | {
    image: (number[] | string);
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt?: string;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
});
export interface AiOutput8 {
    description?: string;
}
export interface AiInput9 {
    /**
     * A text description of the image you want to generate
     */
    prompt: string;
    /**
     * Text describing elements to avoid in the generated image
     */
    negative_prompt?: string;
    /**
     * The height of the generated image in pixels
     */
    height?: number;
    /**
     * The width of the generated image in pixels
     */
    width?: number;
    /**
     * For use with img2img tasks. An array of integers that represent the image data constrained to 8-bit unsigned integer values
     */
    image?: number[];
    /**
     * For use with img2img tasks. A base64-encoded string of the input image
     */
    image_b64?: string;
    /**
     * An array representing An array of integers that represent mask image data for inpainting constrained to 8-bit unsigned integer values
     */
    mask?: number[];
    /**
     * The number of diffusion steps; higher values can improve quality but take longer
     */
    num_steps?: number;
    /**
     * A value between 0 and 1 indicating how strongly to apply the transformation during img2img tasks; lower values make the output closer to the input image
     */
    strength?: number;
    /**
     * Controls how closely the generated image should adhere to the prompt; higher values make the image more aligned with the prompt
     */
    guidance?: number;
    /**
     * Random seed for reproducibility of the image generation
     */
    seed?: number;
}
/**
 * The generated image in PNG format
 */
export type AiOutput9 = string;
export interface AiInput10 {
    audio: {
        body: {};
        contentType: string;
    };
    /**
     * Sets how the model will interpret strings submitted to the custom_topic param. When strict, the model will only return topics submitted using the custom_topic param. When extended, the model will return its own detected topics in addition to those submitted using the custom_topic param.
     */
    custom_topic_mode?: ("extended" | "strict");
    /**
     * Custom topics you want the model to detect within your input audio or text if present Submit up to 100
     */
    custom_topic?: string;
    /**
     * Sets how the model will interpret intents submitted to the custom_intent param. When strict, the model will only return intents submitted using the custom_intent param. When extended, the model will return its own detected intents in addition those submitted using the custom_intents param
     */
    custom_intent_mode?: ("extended" | "strict");
    /**
     * Custom intents you want the model to detect within your input audio if present
     */
    custom_intent?: string;
    /**
     * Identifies and extracts key entities from content in submitted audio
     */
    detect_entities?: boolean;
    /**
     * Identifies the dominant language spoken in submitted audio
     */
    detect_language?: boolean;
    /**
     * Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0
     */
    diarize?: boolean;
    /**
     * Identify and extract key entities from content in submitted audio
     */
    dictation?: boolean;
    /**
     * Specify the expected encoding of your submitted audio
     */
    encoding?: ("linear16" | "flac" | "mulaw" | "amr-nb" | "amr-wb" | "opus" | "speex" | "g729");
    /**
     * Arbitrary key-value pairs that are attached to the API response for usage in downstream processing
     */
    extra?: string;
    /**
     * Filler Words can help transcribe interruptions in your audio, like 'uh' and 'um'
     */
    filler_words?: boolean;
    /**
     * Key term prompting can boost or suppress specialized terminology and brands.
     */
    keyterm?: string;
    /**
     * Keywords can boost or suppress specialized terminology and brands.
     */
    keywords?: string;
    /**
     * The BCP-47 language tag that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available.
     */
    language?: string;
    /**
     * Spoken measurements will be converted to their corresponding abbreviations.
     */
    measurements?: boolean;
    /**
     * Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip.
     */
    mip_opt_out?: boolean;
    /**
     * Mode of operation for the model representing broad area of topic that will be talked about in the supplied audio
     */
    mode?: ("general" | "medical" | "finance");
    /**
     * Transcribe each audio channel independently.
     */
    multichannel?: boolean;
    /**
     * Numerals converts numbers from written format to numerical format.
     */
    numerals?: boolean;
    /**
     * Splits audio into paragraphs to improve transcript readability.
     */
    paragraphs?: boolean;
    /**
     * Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely.
     */
    profanity_filter?: boolean;
    /**
     * Add punctuation and capitalization to the transcript.
     */
    punctuate?: boolean;
    /**
     * Redaction removes sensitive information from your transcripts.
     */
    redact?: string;
    /**
     * Search for terms or phrases in submitted audio and replaces them.
     */
    replace?: string;
    /**
     * Search for terms or phrases in submitted audio.
     */
    search?: string;
    /**
     * Recognizes the sentiment throughout a transcript or text.
     */
    sentiment?: boolean;
    /**
     * Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability.
     */
    smart_format?: boolean;
    /**
     * Detect topics throughout a transcript or text.
     */
    topics?: boolean;
    /**
     * Segments speech into meaningful semantic units.
     */
    utterances?: boolean;
    /**
     * Seconds to wait before detecting a pause between words in submitted audio.
     */
    utt_split?: number;
    /**
     * The number of channels in the submitted audio
     */
    channels?: number;
    /**
     * Specifies whether the streaming endpoint should provide ongoing transcription updates as more audio is received. When set to true, the endpoint sends continuous updates, meaning transcription results may evolve over time. Note: Supported only for webosockets.
     */
    interim_results?: boolean;
    /**
     * Indicates how long model will wait to detect whether a speaker has finished speaking or pauses for a significant period of time. When set to a value, the streaming endpoint immediately finalizes the transcription for the processed time range and returns the transcript with a speech_final parameter set to true. Can also be set to false to disable endpointing
     */
    endpointing?: string;
    /**
     * Indicates that speech has started. You'll begin receiving Speech Started messages upon speech starting. Note: Supported only for webosockets.
     */
    vad_events?: boolean;
    /**
     * Indicates how long model will wait to send an UtteranceEnd message after a word has been transcribed. Use with interim_results. Note: Supported only for webosockets.
     */
    utterance_end_ms?: boolean;
}
export interface AiOutput10 {
    results?: {
        channels?: {
            alternatives?: {
                confidence?: number;
                transcript?: string;
                words?: {
                    confidence?: number;
                    end?: number;
                    start?: number;
                    word?: string;
                }[];
            }[];
        }[];
        summary?: {
            result?: string;
            short?: string;
        };
        sentiments?: {
            segments?: {
                text?: string;
                start_word?: number;
                end_word?: number;
                sentiment?: string;
                sentiment_score?: number;
            }[];
            average?: {
                sentiment?: string;
                sentiment_score?: number;
            };
        };
    };
}
export interface AiInput11 {
    /**
     * A text description of the image you want to generate.
     */
    prompt: string;
    /**
     * The number of diffusion steps; higher values can improve quality but take longer.
     */
    steps?: number;
}
export interface AiOutput11 {
    /**
     * The generated image in Base64 format.
     */
    image?: string;
}
export type AiInput12 = (string | {
    /**
     * An array of integers that represent the image data constrained to 8-bit unsigned integer values
     */
    image: number[];
});
export type AiOutput12 = {
    /**
     * A confidence value, between 0 and 1, indicating how certain the model is about the predicted label
     */
    score?: number;
    /**
     * The predicted category or class for the input image based on analysis
     */
    label?: string;
}[];
export type AiInput13 = (Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Prompt | Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Messages | AsyncBatch);
export interface Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
     */
    lora?: string;
    response_format?: JSONMode;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface _cf_meta_llama_3_3_70b_instruct_fp8_fast_AiInput13_JSONMode {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface Meta_Llama_3_3_70B_Instruct_Fp8_Fast_Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role: string;
        /**
         * The content of the message as a string.
         */
        content: string;
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    response_format?: JSONMode1;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface _cf_meta_llama_3_3_70b_instruct_fp8_fast_AiInput13_JSONMode1 {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface AsyncBatch {
    requests?: {
        /**
         * User-supplied reference. This field will be present in the response as well it can be used to reference the request and response. It's NOT validated to be unique.
         */
        external_reference?: string;
        /**
         * Prompt for the text generation model
         */
        prompt?: string;
        /**
         * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
         */
        stream?: boolean;
        /**
         * The maximum number of tokens to generate in the response.
         */
        max_tokens?: number;
        /**
         * Controls the randomness of the output; higher values produce more random results.
         */
        temperature?: number;
        /**
         * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
         */
        top_p?: number;
        /**
         * Random seed for reproducibility of the generation.
         */
        seed?: number;
        /**
         * Penalty for repeated tokens; higher values discourage repetition.
         */
        repetition_penalty?: number;
        /**
         * Decreases the likelihood of the model repeating the same lines verbatim.
         */
        frequency_penalty?: number;
        /**
         * Increases the likelihood of the model introducing new topics.
         */
        presence_penalty?: number;
        response_format?: JSONMode2;
    }[];
}
export interface JSONMode2 {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface AiOutput13 {
    /**
     * The generated text response from the model
     */
    response: string;
    /**
     * Usage statistics for the inference request
     */
    usage?: {
        /**
         * Total number of tokens in input
         */
        prompt_tokens?: number;
        /**
         * Total number of tokens in output
         */
        completion_tokens?: number;
        /**
         * Total number of input and output tokens
         */
        total_tokens?: number;
    };
    /**
     * An array of tool calls requests made during the response generation
     */
    tool_calls?: {
        /**
         * The arguments passed to be passed to the tool call request
         */
        arguments?: {};
        /**
         * The name of the tool to be called
         */
        name?: string;
    }[];
}
export interface AiInput14 {
    /**
     * A text description of the image you want to generate.
     */
    prompt: string;
    /**
     * Controls how closely the generated image should adhere to the prompt; higher values make the image more aligned with the prompt
     */
    guidance?: number;
    /**
     * Random seed for reproducibility of the image generation
     */
    seed?: number;
    /**
     * The height of the generated image in pixels
     */
    height?: number;
    /**
     * The width of the generated image in pixels
     */
    width?: number;
    /**
     * The number of diffusion steps; higher values can improve quality but take longer
     */
    num_steps?: number;
    /**
     * Specify what to exclude from the generated images
     */
    negative_prompt?: string;
}
/**
 * The generated image in JPEG format
 */
export type AiOutput14 = string;
export type AiInput15 = ({
    /**
     * The text to be translated
     */
    text: string;
    /**
     * The language code of the source text (e.g., 'en' for English). Defaults to 'en' if not specified
     */
    source_lang?: string;
    /**
     * The language code to translate the text into (e.g., 'es' for Spanish)
     */
    target_lang: string;
} | {
    /**
     * Batch of the embeddings requests to run using async-queue
     */
    requests: {
        /**
         * The text to be translated
         */
        text: string;
        /**
         * The language code of the source text (e.g., 'en' for English). Defaults to 'en' if not specified
         */
        source_lang?: string;
        /**
         * The language code to translate the text into (e.g., 'es' for Spanish)
         */
        target_lang: string;
    }[];
});
export interface AiOutput15 {
    /**
     * The translated text in the target language
     */
    translated_text?: string;
}
export type AiInput16 = ({
    text: (string | string[]);
    /**
     * The pooling method used in the embedding process. `cls` pooling will generate more accurate embeddings on larger inputs - however, embeddings created with cls pooling are not compatible with embeddings generated with mean pooling. The default pooling method is `mean` in order for this to not be a breaking change, but we highly suggest using the new `cls` pooling for better accuracy.
     */
    pooling?: ("mean" | "cls");
} | {
    /**
     * Batch of the embeddings requests to run using async-queue
     */
    requests: {
        text: (string | string[]);
        /**
         * The pooling method used in the embedding process. `cls` pooling will generate more accurate embeddings on larger inputs - however, embeddings created with cls pooling are not compatible with embeddings generated with mean pooling. The default pooling method is `mean` in order for this to not be a breaking change, but we highly suggest using the new `cls` pooling for better accuracy.
         */
        pooling?: ("mean" | "cls");
    }[];
});
export interface AiOutput16 {
    shape?: number[];
    /**
     * Embeddings of the requested text values
     */
    data?: number[][];
    /**
     * The pooling method used in the embedding process.
     */
    pooling?: ("mean" | "cls");
}
export type AiInput17 = (Qwen2_5_Coder_32B_Instruct_Prompt | Qwen2_5_Coder_32B_Instruct_Messages);
export interface Qwen2_5_Coder_32B_Instruct_Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
     */
    lora?: string;
    response_format?: JSONMode;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface _cf_qwen_qwen2_5_coder_32b_instruct_AiInput17_JSONMode {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface Qwen2_5_Coder_32B_Instruct_Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role: string;
        /**
         * The content of the message as a string.
         */
        content: string;
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    response_format?: JSONMode1;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface _cf_qwen_qwen2_5_coder_32b_instruct_AiInput17_JSONMode1 {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export type AiInput18 = (string | {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt?: string;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
    image: (number[] | string);
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
});
export type AiInput19 = (Mistral_Small_3_1_24B_Instruct_Prompt | Mistral_Small_3_1_24B_Instruct_Messages);
export interface Mistral_Small_3_1_24B_Instruct_Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * JSON schema that should be fulfilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface Mistral_Small_3_1_24B_Instruct_Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role?: string;
        /**
         * The tool call id. Must be supplied for tool calls for Mistral-3. If you don't know what to put here you can fall back to 000000001
         */
        tool_call_id?: string;
        content?: (string | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        }[] | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        });
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    /**
     * JSON schema that should be fufilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface AiOutput17 {
    /**
     * The generated text response from the model
     */
    response: string;
    /**
     * Usage statistics for the inference request
     */
    usage?: {
        /**
         * Total number of tokens in input
         */
        prompt_tokens?: number;
        /**
         * Total number of tokens in output
         */
        completion_tokens?: number;
        /**
         * Total number of input and output tokens
         */
        total_tokens?: number;
    };
    /**
     * An array of tool calls requests made during the response generation
     */
    tool_calls?: {
        /**
         * The arguments passed to be passed to the tool call request
         */
        arguments?: {};
        /**
         * The name of the tool to be called
         */
        name?: string;
    }[];
}
export type AiInput20 = (_cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Prompt | _cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Messages);
export interface _cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    image?: (number[] | string);
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
    /**
     * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
     */
    lora?: string;
}
export interface _cf_meta_llama_3_2_11b_vision_instruct_AiInput20_Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role?: string;
        /**
         * The tool call id. Must be supplied for tool calls for Mistral-3. If you don't know what to put here you can fall back to 000000001
         */
        tool_call_id?: string;
        content?: (string | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        }[] | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        });
    }[];
    image?: (number[] | string);
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    /**
     * If true, the response will be streamed back incrementally.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface AiOutput18 {
    /**
     * The generated text response from the model
     */
    response?: string;
    /**
     * An array of tool calls requests made during the response generation
     */
    tool_calls?: {
        /**
         * The arguments passed to be passed to the tool call request
         */
        arguments?: {};
        /**
         * The name of the tool to be called
         */
        name?: string;
    }[];
}
export interface AiInput21 {
    /**
     * Base64 encoded value of the audio data.
     */
    audio: string;
    /**
     * Supported tasks are 'translate' or 'transcribe'.
     */
    task?: string;
    /**
     * The language of the audio being transcribed or translated.
     */
    language?: string;
    /**
     * Preprocess the audio with a voice activity detection model.
     */
    vad_filter?: boolean;
    /**
     * A text prompt to help provide context to the model on the contents of the audio.
     */
    initial_prompt?: string;
    /**
     * The prefix it appended the the beginning of the output of the transcription and can guide the transcription result.
     */
    prefix?: string;
}
export interface AiOutput19 {
    transcription_info?: {
        /**
         * The language of the audio being transcribed or translated.
         */
        language?: string;
        /**
         * The confidence level or probability of the detected language being accurate, represented as a decimal between 0 and 1.
         */
        language_probability?: number;
        /**
         * The total duration of the original audio file, in seconds.
         */
        duration?: number;
        /**
         * The duration of the audio after applying Voice Activity Detection (VAD) to remove silent or irrelevant sections, in seconds.
         */
        duration_after_vad?: number;
    };
    /**
     * The complete transcription of the audio.
     */
    text: string;
    /**
     * The total number of words in the transcription.
     */
    word_count?: number;
    segments?: {
        /**
         * The starting time of the segment within the audio, in seconds.
         */
        start?: number;
        /**
         * The ending time of the segment within the audio, in seconds.
         */
        end?: number;
        /**
         * The transcription of the segment.
         */
        text?: string;
        /**
         * The temperature used in the decoding process, controlling randomness in predictions. Lower values result in more deterministic outputs.
         */
        temperature?: number;
        /**
         * The average log probability of the predictions for the words in this segment, indicating overall confidence.
         */
        avg_logprob?: number;
        /**
         * The compression ratio of the input to the output, measuring how much the text was compressed during the transcription process.
         */
        compression_ratio?: number;
        /**
         * The probability that the segment contains no speech, represented as a decimal between 0 and 1.
         */
        no_speech_prob?: number;
        words?: {
            /**
             * The individual word transcribed from the audio.
             */
            word?: string;
            /**
             * The starting time of the word within the audio, in seconds.
             */
            start?: number;
            /**
             * The ending time of the word within the audio, in seconds.
             */
            end?: number;
        }[];
    }[];
    /**
     * The transcription in WebVTT format, which includes timing and text information for use in subtitles.
     */
    vtt?: string;
}
export interface AiInput22 {
    /**
     * Speaker used to produce the audio.
     */
    speaker?: ("angus" | "asteria" | "arcas" | "orion" | "orpheus" | "athena" | "luna" | "zeus" | "perseus" | "helios" | "hera" | "stella");
    /**
     * Encoding of the output audio.
     */
    encoding?: ("linear16" | "flac" | "mulaw" | "alaw" | "mp3" | "opus" | "aac");
    /**
     * Container specifies the file format wrapper for the output audio. The available options depend on the encoding type..
     */
    container?: ("none" | "wav" | "ogg");
    /**
     * The text content to be converted to speech
     */
    text: string;
    /**
     * Sample Rate specifies the sample rate for the output audio. Based on the encoding, different sample rates are supported. For some encodings, the sample rate is not configurable
     */
    sample_rate?: number;
    /**
     * The bitrate of the audio in bits per second. Choose from predefined ranges or specific values based on the encoding type.
     */
    bit_rate?: number;
}
/**
 * The generated audio in MP3 format
 */
export type AiOutput20 = string;
export interface AiInput23 {
    /**
     * The text that you want the model to summarize
     */
    input_text: string;
    /**
     * The maximum length of the generated summary in tokens
     */
    max_length?: number;
}
export interface AiOutput21 {
    /**
     * The summarized version of the input text
     */
    summary?: string;
}
export interface AiInput24 {
    text: (string | string[]);
}
export interface AiOutput22 {
    shape?: number[];
    /**
     * Embeddings of the requested text values
     */
    data?: number[][];
}
export interface AiInput25 {
    /**
     * A query you wish to perform against the provided contexts.
     */
    query: string;
    /**
     * Number of returned results starting with the best score.
     */
    top_k?: number;
    /**
     * List of provided contexts. Note that the index in this array is important, as the response will refer to it.
     */
    contexts: {
        /**
         * One of the provided context content
         */
        text?: string;
    }[];
}
export interface AiOutput23 {
    response?: {
        /**
         * Index of the context in the request
         */
        id?: number;
        /**
         * Score of the context under the index.
         */
        score?: number;
    }[];
}
export interface AiInput26 {
    /**
     * A text description of the image you want to generate.
     */
    prompt: string;
    /**
     * Controls how closely the generated image should adhere to the prompt; higher values make the image more aligned with the prompt
     */
    guidance?: number;
    /**
     * Random seed for reproducibility of the image generation
     */
    seed?: number;
    /**
     * The height of the generated image in pixels
     */
    height?: number;
    /**
     * The width of the generated image in pixels
     */
    width?: number;
    /**
     * The number of diffusion steps; higher values can improve quality but take longer
     */
    num_steps?: number;
    /**
     * The number of diffusion steps; higher values can improve quality but take longer
     */
    steps?: number;
}
export type AiInput27 = (Ai_Cf_Meta_Llama_4_Prompt | Ai_Cf_Meta_Llama_4_Messages | Ai_Cf_Meta_Llama_4_Async_Batch);
export interface Ai_Cf_Meta_Llama_4_Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * JSON schema that should be fulfilled for the response.
     */
    guided_json?: {};
    response_format?: JSONMode;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface _cf_meta_llama_4_scout_17b_16e_instruct_AiInput27_JSONMode {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface Ai_Cf_Meta_Llama_4_Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role?: string;
        /**
         * The tool call id. If you don't know what to put here you can fall back to 000000001
         */
        tool_call_id?: string;
        content?: (string | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        }[] | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        });
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    response_format?: JSONMode1;
    /**
     * JSON schema that should be fufilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface _cf_meta_llama_4_scout_17b_16e_instruct_AiInput27_JSONMode1 {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface Ai_Cf_Meta_Llama_4_Async_Batch {
    requests: (Ai_Cf_Meta_Llama_4_Prompt_Inner | Ai_Cf_Meta_Llama_4_Messages_Inner)[];
}
export interface Ai_Cf_Meta_Llama_4_Prompt_Inner {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * JSON schema that should be fulfilled for the response.
     */
    guided_json?: {};
    response_format?: JSONMode2;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface _cf_meta_llama_4_scout_17b_16e_instruct_AiInput27_JSONMode2 {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface Ai_Cf_Meta_Llama_4_Messages_Inner {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role?: string;
        /**
         * The tool call id. If you don't know what to put here you can fall back to 000000001
         */
        tool_call_id?: string;
        content?: (string | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        }[] | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        });
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    response_format?: JSONMode3;
    /**
     * JSON schema that should be fufilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface JSONMode3 {
    type?: ("json_object" | "json_schema");
    json_schema?: any;
}
export interface AiOutput24 {
    /**
     * The generated text response from the model
     */
    response: string;
    /**
     * Usage statistics for the inference request
     */
    usage?: {
        /**
         * Total number of tokens in input
         */
        prompt_tokens?: number;
        /**
         * Total number of tokens in output
         */
        completion_tokens?: number;
        /**
         * Total number of input and output tokens
         */
        total_tokens?: number;
    };
    /**
     * An array of tool calls requests made during the response generation
     */
    tool_calls?: {
        /**
         * The tool call id.
         */
        id?: string;
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type?: string;
        /**
         * Details of the function tool.
         */
        function?: {
            /**
             * The name of the tool to be called
             */
            name?: string;
            /**
             * The arguments passed to be passed to the tool call request
             */
            arguments?: {};
        };
    }[];
}
export type AiInput28 = (Google_Gemma_3_12B_It_Prompt | Google_Gemma_3_12B_It_Messages);
export interface Google_Gemma_3_12B_It_Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * JSON schema that should be fufilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface Google_Gemma_3_12B_It_Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role?: string;
        content?: (string | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        }[] | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        });
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    /**
     * JSON schema that should be fufilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export type AiInput29 = (Qwen_Qwq_32B_Prompt | Qwen_Qwq_32B_Messages);
export interface Qwen_Qwq_32B_Prompt {
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * JSON schema that should be fulfilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export interface Qwen_Qwq_32B_Messages {
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role?: string;
        /**
         * The tool call id. Must be supplied for tool calls for Mistral-3. If you don't know what to put here you can fall back to 000000001
         */
        tool_call_id?: string;
        content?: (string | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        }[] | {
            /**
             * Type of the content provided
             */
            type?: string;
            text?: string;
            image_url?: {
                /**
                 * image uri with data (e.g. data:image/jpeg;base64,/9j/...). HTTP URL will not be accepted
                 */
                url?: string;
            };
        });
    }[];
    functions?: {
        name: string;
        code: string;
    }[];
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                };
            };
        };
    } | {
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * The name of the function.
             */
            name: string;
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                    };
                };
            };
        };
    })[];
    /**
     * JSON schema that should be fufilled for the response.
     */
    guided_json?: {};
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
}
export type AiInput30 = (_cf_meta_llama_3_1_70b_instruct_AiInput30_Prompt | _cf_meta_llama_3_1_70b_instruct_AiInput30_Messages);
export interface _cf_meta_llama_3_1_70b_instruct_AiInput30_Prompt {
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    image?: (number[] | string);
    /**
     * Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.
     */
    lora?: string;
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
    /**
     * The input text prompt for the model to generate a response.
     */
    prompt: string;
    /**
     * If true, a chat template is not applied and you must adhere to the specific model's expected formatting.
     */
    raw?: boolean;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * If true, the response will be streamed back incrementally using SSE, Server Sent Events.
     */
    stream?: boolean;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
}
export interface _cf_meta_llama_3_1_70b_instruct_AiInput30_Messages {
    /**
     * Decreases the likelihood of the model repeating the same lines verbatim.
     */
    frequency_penalty?: number;
    functions?: {
        code: string;
        name: string;
    }[];
    image?: (number[] | string);
    /**
     * The maximum number of tokens to generate in the response.
     */
    max_tokens?: number;
    /**
     * An array of message objects representing the conversation history.
     */
    messages: {
        /**
         * The content of the message as a string.
         */
        content: string;
        /**
         * The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').
         */
        role: string;
    }[];
    /**
     * Increases the likelihood of the model introducing new topics.
     */
    presence_penalty?: number;
    /**
     * Penalty for repeated tokens; higher values discourage repetition.
     */
    repetition_penalty?: number;
    /**
     * Random seed for reproducibility of the generation.
     */
    seed?: number;
    /**
     * If true, the response will be streamed back incrementally.
     */
    stream?: boolean;
    /**
     * Controls the randomness of the output; higher values produce more random results.
     */
    temperature?: number;
    /**
     * A list of tools available for the assistant to use.
     */
    tools?: ({
        /**
         * A brief description of what the tool does.
         */
        description: string;
        /**
         * The name of the tool. More descriptive the better.
         */
        name: string;
        /**
         * Schema defining the parameters accepted by the tool.
         */
        parameters: {
            /**
             * Definitions of each parameter.
             */
            properties: {
                [k: string]: {
                    /**
                     * A description of the expected parameter.
                     */
                    description: string;
                    /**
                     * The data type of the parameter.
                     */
                    type: string;
                };
            };
            /**
             * List of required parameter names.
             */
            required?: string[];
            /**
             * The type of the parameters object (usually 'object').
             */
            type: string;
        };
    } | {
        /**
         * Details of the function tool.
         */
        function: {
            /**
             * A brief description of what the function does.
             */
            description: string;
            /**
             * The name of the function.
             */
            name: string;
            /**
             * Schema defining the parameters accepted by the function.
             */
            parameters: {
                /**
                 * Definitions of each parameter.
                 */
                properties: {
                    [k: string]: {
                        /**
                         * A description of the expected parameter.
                         */
                        description: string;
                        /**
                         * The data type of the parameter.
                         */
                        type: string;
                    };
                };
                /**
                 * List of required parameter names.
                 */
                required?: string[];
                /**
                 * The type of the parameters object (usually 'object').
                 */
                type: string;
            };
        };
        /**
         * Specifies the type of tool (e.g., 'function').
         */
        type: string;
    })[];
    /**
     * Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.
     */
    top_k?: number;
    /**
     * Controls the creativity of the AI's responses by adjusting how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.
     */
    top_p?: number;
}
export interface AiInput31 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiOutput25 {
    choices: {
        message: {
            role: "assistant";
            content: string;
        };
        finish_reason?: string;
    }[];
    usage?: {
        prompt_tokens?: number;
        completion_tokens?: number;
        total_tokens?: number;
    };
}
export interface AiInput32 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput33 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput34 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput35 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput36 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput37 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput38 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput39 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput40 {
    /**
     * Audio file to transcribe
     */
    file: string;
    /**
     * Model to use for transcription
     */
    model?: string;
    /**
     * Language of the audio
     */
    language?: string;
    /**
     * Optional prompt to guide transcription
     */
    prompt?: string;
    response_format?: ("json" | "text" | "srt" | "verbose_json" | "vtt");
    temperature?: number;
}
export interface AiOutput26 {
    /**
     * Transcribed text
     */
    text: string;
}
export interface AiInput41 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput42 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput43 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput44 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput45 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput46 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
export interface AiInput47 {
    /**
     * Text to embed
     */
    input: (string | string[]);
    /**
     * Model to use for embeddings
     */
    model?: string;
    encoding_format?: ("float" | "base64");
    /**
     * Number of dimensions in the embedding
     */
    dimensions?: number;
}
export interface AiOutput27 {
    data: {
        embedding: number[];
        index: number;
    }[];
    model?: string;
    usage?: {
        prompt_tokens?: number;
        total_tokens?: number;
    };
}
export interface AiInput48 {
    messages: {
        role: ("system" | "user" | "assistant");
        content: string;
    }[];
    model?: string;
    temperature?: number;
    max_tokens?: number;
    stream?: boolean;
    response_format?: {
        type?: ("json_object" | "text");
    };
}
/** Unified input type mapping using conditional types based on model catalog */
export type AiModelInputMap = {
    [K in AvailableModel]: K extends 'pii-detection' ? {
        prompt: string;
    } : K extends 'bge-m3' | 'bge-small-en-v1.5' | 'bge-base-en-v1.5' | 'embeddinggemma-300m' | 'bge-large-en-v1.5' | 'embeddings' ? {
        input: string | string[];
        model?: string;
        encoding_format?: 'float' | 'base64';
        dimensions?: number;
    } : K extends 'melotts' | 'aura-1' ? {
        text: string;
        model?: string;
        voice?: string;
        speed?: number;
        response_format?: 'mp3' | 'opus' | 'aac' | 'flac' | 'wav' | 'pcm';
    } : K extends 'smart-turn-v2' | 'whisper' | 'nova-3' | 'whisper-tiny-en' | 'whisper-large-v3-turbo' | 'whisper-large-v3' ? AudioInput : K extends 'stable-diffusion-v1-5-inpainting' | 'flux-1-schnell' | 'stable-diffusion-xl-lightning' | 'dreamshaper-8-lcm' | 'phoenix-1.0' | 'stable-diffusion-xl-base-1.0' | 'stable-diffusion-v1-5-img2img' | 'lucid-origin' ? ImageGenerationInput : K extends 'distilbert-sst-2-int8' | 'bge-reranker-base' ? TextClassificationInput : K extends 'resnet-50' ? ImageClassificationInput : K extends 'm2m100-1.2b' ? TranslationInput : K extends 'bart-large-cnn' ? SummarizationInput : K extends 'llava-1.5-7b-hf' | 'uform-gen2-qwen-500m' | 'llama-3.2-11b-vision-instruct' | 'llama-4-maverick-17b' ? VisionInput : K extends 'llama-3.3-70b' | 'llama-3.1-8b-external' | 'llama-3.1-70b-instruct' | 'deepseek-r1' | 'deepseek-v3-0324' | 'deepseek-r1-distill-llama-70b' | 'qwen-3-32b' | 'llama-3.3-swallow-70b' | 'llama-3.1-8b-instant' | 'gemma-9b-it' | 'kimi-k2' | 'gpt-oss-120b' | 'gpt-oss-20b' | 'gpt-oss-120b-test' ? OpenAIChatInputExternal : OpenAIChatInput;
};
/** Unified output type mapping using conditional types based on model catalog */
export type AiModelOutputMap = {
    [K in AvailableModel]: K extends 'pii-detection' ? {
        pii_detection: Array<{
            entity_type: string;
            text: string;
            start: number;
            end: number;
            confidence: number;
        }>;
    } : K extends 'bge-m3' | 'bge-small-en-v1.5' | 'bge-base-en-v1.5' | 'embeddinggemma-300m' | 'bge-large-en-v1.5' | 'embeddings' ? OpenAIEmbeddingOutput : K extends 'melotts' | 'aura-1' ? {
        audio: ArrayBuffer | Uint8Array;
        response_format?: string;
    } : K extends 'smart-turn-v2' | 'whisper' | 'nova-3' | 'whisper-tiny-en' | 'whisper-large-v3-turbo' | 'whisper-large-v3' ? OpenAIAudioOutput : K extends 'stable-diffusion-v1-5-inpainting' | 'flux-1-schnell' | 'stable-diffusion-xl-lightning' | 'dreamshaper-8-lcm' | 'phoenix-1.0' | 'stable-diffusion-xl-base-1.0' | 'stable-diffusion-v1-5-img2img' | 'lucid-origin' ? ImageGenerationOutput : K extends 'distilbert-sst-2-int8' | 'bge-reranker-base' ? TextClassificationOutput : K extends 'resnet-50' ? ImageClassificationOutput : K extends 'm2m100-1.2b' ? TranslationOutput : K extends 'bart-large-cnn' ? SummarizationOutput : K extends 'llava-1.5-7b-hf' | 'uform-gen2-qwen-500m' | 'llama-3.2-11b-vision-instruct' | 'llama-4-maverick-17b' ? VisionOutput : OpenAIChatOutput;
};
/** Union of all available AI models */
export type AiModel = AvailableModel;
/** Generic async response type for queued AI requests */
export interface AiAsyncResponse {
    /** The async request id that can be used to obtain the results */
    request_id: string;
}
/** Main interface for AI operations
 * @remarks
 * Provides a unified interface for running various AI models across different tasks.
 * Each model type has its own specific input and output types.
 */
export interface Ai {
    run<T extends AiModel, I extends AiModelInputMap[T] = AiModelInputMap[T], O extends ExtendedAiOptions = ExtendedAiOptions>(model: T, inputs: I, options?: O): Promise<I extends {
        stream: true;
    } ? ReadableStream : O extends {
        returnRawResponse: true;
    } ? Response : O extends {
        queueRequest: true;
    } ? AiAsyncResponse : AiModelOutputMap[T]>;
}
//# sourceMappingURL=ai.d.ts.map